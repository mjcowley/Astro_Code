{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    Introduction to Python for Astronomers Notebook\n",
    "  </h1>\n",
    "</div>"
   ],
   "id": "b43a9c5f6e31fb1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Last updated: 11 Dec 2024\n",
    "\n",
    "**This tutorial was created by Michael Cowley with inspiration from “Python for Astronomers” by Imad Pasha.**\n",
    "\n",
    "<br>\n",
    "<img src=\"https://mjcowley.github.io/images/python.png\" alt=\"Python Logo\" width=\"400\"/>\n",
    "\n",
    "Welcome to this introductory notebook designed to familiarise newcomers with the essentials of [Python](https://www.python.org/), particularly in the contexts of astronomy and astrophysics. Python is renowned for its readability, simplicity, and the extensive range of libraries it offers, such as [NumPy](https://numpy.org/) for numerical computations, [pandas](https://pandas.pydata.org/) for data manipulation, and [Matplotlib](https://matplotlib.org/) for visualisation. These tools significantly simplify the tasks of data analysis and visualisation. This guide aims to introduce you to the fundamental concepts of Python, showcase these critical libraries, and their practical uses within Google Colab.\n",
    "\n",
    "<br>\n",
    "<img src=\"https://mjcowley.github.io/images/colab.png\" alt=\"Colab Logo\" width=\"400\"/>\n",
    "\n",
    "**Google Colab**, or **Colaboratory**, presented by Google Research, is a cloud-based [Jupyter notebook](https://jupyter.org/) service that allows the execution of Python (and other languages) directly in your web browser, with no setup required. It’s a valuable resource for collaborative projects, education, or any data science projects, thanks to its ease of use and no-cost access. Colab integrates seamlessly with Google Drive, making it straightforward to store, access, and share your datasets, notebooks, and other files.\n",
    "\n",
    "For beginners to cloud-based development environments, starting with Google Colab is an excellent introduction to the world of collaborative coding platforms. Utilising a Google account to store and manage your datasets in Google Colab is perhaps the most straightforward method for those new to the platform. If you don’t have a Google account yet, you can sign up for free [here](https://www.google.com/account/about/).\n",
    "\n",
    "However, **postgraduate students or more advanced users** of Python are encouraged to use their own Integrated Development Environment (IDE) for more complex projects and greater flexibility in development. Examples of powerful IDEs for Python include:\n",
    "- [**Visual Studio Code** (VS Code)](https://code.visualstudio.com/)\n",
    "- [**PyCharm**](https://www.jetbrains.com/pycharm/)\n",
    "- [**Spyder**](https://www.spyder-ide.org/)\n",
    "\n",
    "These IDEs offer advanced features such as debugging, code linting, and version control integration, which are useful for larger or more complex coding tasks.\n",
    "\n",
    "Finally, be sure to [watch the instructional video here](https://www.youtube.com/watch?v=inN8seMm7UI) if you need a refresher on Google Colab.\n"
   ],
   "id": "fd2f3401ec56be0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Basic Operations",
   "id": "e90d20f8238eed17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's begin by understanding what a *declaration* is. Python acts as a highly advanced calculator designed for complex calculations, and many Python programs are sequences of simple mathematical operations executed on data. To handle more intricate data than what you'd input into a standard calculator, Python enables the creation of variables. These variables store values for future reference. Run each of the cells in sequence below:",
   "id": "74b6dd6807f9a19d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "variable_1 = 2",
   "id": "44f2172e585a03e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "variable_2 = 3",
   "id": "e2779ca5ff4aa146",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "output_1 = variable_1 + variable_2",
   "id": "8d8f603dcdbb7c3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(output_1)",
   "id": "20693f7625690ce5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I simply saved the numbers 2 and 3 into variables, choosing the basic names 'variable_1' and 'variable_2', and then calculated their sum. Notice the underscore in the variable names. **Spaces_aren't_allowed_in_variable_names**, so underscores are commonly used instead. We'll discuss variable naming best practices later on.\n",
    "\n",
    "You might wonder, ''Why bother declaring those variables and then adding them, instead of directly calculating:''"
   ],
   "id": "76b9590238d5e708"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "2+3",
   "id": "953a5167a40fd60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this instance, you're spot on. If my goal was simply to find the sum of 2 and 3, I could have directly inputted it. Moreover, should there have been a need to store that result, I could have easily done so:",
   "id": "6b19d87321a44b19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "output_2 = 2+3",
   "id": "5851b25f3f192411",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And now, I can at any time look at that:",
   "id": "7abf5ae9093a2120"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(output_2)",
   "id": "4faf78a5e22bd7e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Or, I can use it in further calculations:",
   "id": "2513db9628e6194a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "output_3 = output_1 + output_2",
   "id": "519d4ca7672278a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(output_3)",
   "id": "74507621ce72e2c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is a great opportunity to explore the basic mathematical operations Python offers, two of which we've already encountered. The following lists the basic mathematical operations in Python:",
   "id": "f4a43a643467a43b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Perform basic mathematical operations <- this is a comment, which the cell ignores! Use the \"#\" symbol to comment in Python.\n",
    "addition = 5 + 5\n",
    "subtraction = 5 - 5\n",
    "multiplication = 5 * 5\n",
    "division = 5 / 6 \n",
    "exponentiation = 5**2\n",
    "modulus = 5 % 3"
   ],
   "id": "bfd0635d4d2162c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print results with labels\n",
    "print(\"Addition: \", addition)\n",
    "print(\"Subtraction: \", subtraction)\n",
    "print(\"Multiplication: \", multiplication)\n",
    "print(\"Division: \", division)\n",
    "print(\"Exponentiation: \", exponentiation)\n",
    "print(\"Modulus: \", modulus)"
   ],
   "id": "7f44eff9f938190c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    Exercise: Basic Operations\n",
    "  </h1>\n",
    "</div>"
   ],
   "id": "bf443f31b5828ad8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the box below, create three variables which hold your age and the ages of two other people you know. Then, set a variable named \"age_average\" that is equal to the average of your three ages. Be careful of order of operations! You can group operations, just like in [PEMDAS](https://www.mathsisfun.com/operation-order-pemdas.html) math, using soft parenthesis \"()\".",
   "id": "66fdd16082696605"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "7bd4fb15ac769a23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    End of Exercise\n",
    "  </h1>\n",
    "</div>"
   ],
   "id": "ce0dcc962f0d5ff5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comments\n",
    "Comments are an essential aspect of programming, allowing you to annotate your code with explanations, instructions, or reminders. Comments are ignored by the Python interpreter, meaning they won't affect your code's execution. Comments are preceded by the `#` symbol, which tells Python to disregard the text that follows. Let's see an example:"
   ],
   "id": "1f538cd911629320"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This is a comment\n",
    "print(\"This is not a comment\")"
   ],
   "id": "5efa696f6569f69d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Types in Python",
   "id": "7ad73f341ec69916"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Until now, we've only dealt with numeric data types, namely integers and floats. To check the data type of a variable at any given time, you can use the `type()` function.\n",
   "id": "a46d3a1c670c8aa5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T03:17:08.539507Z",
     "start_time": "2024-12-12T03:17:08.533550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = 2\n",
    "y = 3.0\n",
    "print(type(x))\n",
    "print(type(y))"
   ],
   "id": "13e80203481630c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Casting\n",
    "Python allows you to convert data types using a process called **casting**. This is particularly useful when you need to change a variable's type to perform a specific operation. For example, you can convert an integer to a float, or vice versa. Let's see how this works:"
   ],
   "id": "d7893e7c4b3f7f41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T03:17:54.226726Z",
     "start_time": "2024-12-12T03:17:54.220639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Casting an integer to a float\n",
    "x = 2 # This is an integer\n",
    "y = float(x)\n",
    "print(y)"
   ],
   "id": "4fabbdb196ccd37a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Casting an integer to a float is straightforward. The `float()` function converts the integer `2` to a float, resulting in `2.0`. Similarly, you can convert a float to an integer by using the `int()` function:",
   "id": "282476566326efb2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T03:19:07.094891Z",
     "start_time": "2024-12-12T03:19:07.088717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Casting a float to an integer\n",
    "x = 3.0 # This is a float\n",
    "y = int(x)\n",
    "print(y)"
   ],
   "id": "910b8f988c24feec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "While fundamentally, your data might be represented as numbers, Python's versatility is showcased through its various data types designed for organising numbers and more. Below are Python's essential data types:\n",
    "\n",
    "- **Integers**: Whole numbers without a fractional part.\n",
    "- **Floats**: Numbers with a decimal point.\n",
    "- **Booleans**: `True` or `False` values.\n",
    "- **Lists**: Ordered collections of items.\n",
    "- **Dictionaries**: Collections of items accessed by unique keys.\n",
    "- **Strings**: Textual data enclosed in quotes.\n",
    "- **Tuples**: Ordered collections like lists but immutable.\n",
    "\n",
    "In the upcoming sections, we'll dive into these data types, except for integers and floats, which we have already discussed above.\n"
   ],
   "id": "251ed0ebd07eaeb9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Booleans",
   "id": "b5df0465b2caa245"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Booleans can only be in one of two states: `True` or `False`. Assign a variable to `True` or `False` below, and notice how Python syntax highlights these keywords, indicating their special role.\n",
   "id": "72bdde8d144a9286"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assigning a boolean value to a variable\n",
    "is_sunny = True\n",
    "\n",
    "# Checking the value\n",
    "print(is_sunny)"
   ],
   "id": "c086bee743a4a1da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Booleans are incredibly useful in **conditional statements**, where we tell the code: \"If a certain condition is `True`, do 'X'; else, if another condition is `True`, do 'Y'.\" \n",
    "\n",
    "Often, we're using booleans without even realising it."
   ],
   "id": "3204b7719ebecfe8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Lists",
   "id": "92d0e9fbbc032eee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lists are Python's most versatile containers. You can put nearly anything inside a list, including different data types or even other lists. However, the practicality of mixed-type lists can be limited — the benefit of storing a series of numbers in a list is that you can perform operations on them collectively without concern for compatibility issues. Here's how to create a list:\n",
   "id": "d8d2f2cc5927703e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Defining a list\n",
    "my_list = [1, \"a_string\", True, 3.14, [2, 4, 6]]\n",
    "\n",
    "# Printing the list\n",
    "print(my_list)"
   ],
   "id": "619361c90e1e5cf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The above list is somewhat chaotic – it's rare to want or need a list with such diverse contents. However, it serves to demonstrate that Python is flexible about the types of items you can include in a list. Beyond manually specifying list contents, Python offers functions that can automatically generate lists, especially useful for creating sequences with a regular pattern:\n",
   "id": "8cfab7b20f3e33e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate a list with a regular form using a list comprehension\n",
    "even_numbers = [x for x in range(2, 21, 2)]\n",
    "\n",
    "# Printing the list\n",
    "print(even_numbers)"
   ],
   "id": "d8c6d0571169e473",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This Python snippet demonstrates the use of a list comprehension to generate a list of even numbers. Here's how it works:\n",
    "\n",
    "- `range(2, 21, 2)` creates a sequence of numbers starting from 2 up to (but not including) 21, with a step of 2. This step ensures that only even numbers are included.\n",
    "- `[x for x in range(2, 21, 2)]` iterates over each number `x` in the sequence, adding `x` to the list.\n",
    "- The result is a list of even numbers from 2 to 20, which is then printed out.\n",
    "\n",
    "List comprehensions offer a concise way to create lists, making this method both efficient and easy to read.\n"
   ],
   "id": "899cf36270f2717"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    Exercise: Lists\n",
    "  </h1>\n",
    "</div>"
   ],
   "id": "a0238acce51e7c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Generate a list below containing the odd numbers 1, 3, 5, 7, ... 99 and save it into a variable called odd_count. Then, below, print it to verify your solution.",
   "id": "6abb3840d82c37c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "9ea353d5c981914e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    End of Exercise\n",
    "  </h1>"
   ],
   "id": "10f33476dc9d4bef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Indexing and Slicing",
   "id": "a962f617fbb3b575"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To view the contents of a list, we simply print it:\n",
   "id": "73b374c43cdb0e0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "my_list[0]",
   "id": "b1042fd3c0e6ba12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We put closed brackets at the end of the variable name and specify which index we want. We can also pull multiple elements at once by specifying a range of indices (the first number is inclusive, the second is exclusive):",
   "id": "c7255d8dc09eaf25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "my_list[0:2]",
   "id": "79d99bab49b4ac26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can also specify a skip value, which is the third number in the brackets. This number tells Python how many elements to skip between each element it pulls:",
   "id": "f71f061aff2d6eda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "my_list[0:5:2]",
   "id": "c1dc53223afabc2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "What if we need to access an element within a nested list [2, 4, 6]? This situation calls for **double indexing**. Additionally, this is a good opportunity to introduce **negative indexing**, a handy feature that allows counting backwards from the end of a list, simplifying access to its latter elements:\n",
   "id": "ae67f2b29b4bf4e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "my_list[-1][1]",
   "id": "720544209bd9b579",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The above code snippet demonstrates how to access the number 4 within the nested list [2, 4, 6]. The negative index `-1` refers to the last element in the list, which is itself a list. The index `[1]` then extracts the second element from that list.",
   "id": "d2278492cbf519d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So, to summarise, indexing allows you to access individual elements of an iterable, such as a list or a string, using their position.\n",
    "\n",
    "- **Positive Indexing** starts from 0 at the beginning of the iterable and increases by 1 for each subsequent element.\n",
    "- **Negative Indexing** starts from -1 for the last element, -2 for the second last, and so on, making it easy to access elements from the end.\n",
    "\n",
    "Here's a table illustrating both positive and negative indexing for the string `\"Python\"`:\n",
    "\n",
    "| Index | Character |\n",
    "|-------|-----------|\n",
    "| 0     | P         |\n",
    "| 1     | y         |\n",
    "| 2     | t         |\n",
    "| 3     | h         |\n",
    "| 4     | o         |\n",
    "| 5     | n         |\n",
    "| -6    | P         |\n",
    "| -5    | y         |\n",
    "| -4    | t         |\n",
    "| -3    | h         |\n",
    "| -2    | o         |\n",
    "| -1    | n         |\n",
    "\n",
    "This table demonstrates how each character in the string can be accessed using both positive and negative indices.\n"
   ],
   "id": "845614d0df34b8ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    Exercise: Indexing\n",
    "  </h1>\n",
    "</div>"
   ],
   "id": "e5da730261d088b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the cell below, using our list named my_list that contains an element \"a_string\" at a certain position, write a Python command to output the letter 's' from the string \"a_string\".",
   "id": "73f2c7e6752e082"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "75d983eb733900fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    End of Exercise\n",
    "  </h1>"
   ],
   "id": "db5ddb189410681e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dictionaries",
   "id": "dadeafe9f8b642e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dictionaries function similarly to lists, but instead of using numeric indices to access elements, we use unique \"keys\" to associate with each \"value\". Here's an example:\n",
   "id": "bfd423c3cf97476e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a dictionary\n",
    "my_dict = {\"name\": \"Alice\", \"age\": 30, \"city\": \"Sydney\"}\n",
    "\n",
    "# Accessing a value by its key\n",
    "print(my_dict[\"name\"])"
   ],
   "id": "d5ee25c36ff619f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This Python snippet demonstrates how to create a dictionary and access its elements. Dictionaries store data in key-value pairs, allowing you to quickly access a value by specifying its corresponding key:\n",
    "\n",
    "- We define `my_dict` with three key-value pairs, mapping \"name\" to \"Alice\", \"age\" to 30, and \"city\" to \"Sydney\".\n",
    "- To retrieve a value, such as Alice's name, we use the syntax `my_dict[\"name\"]`, specifying the key to obtain its associated value.\n",
    "\n",
    "Dictionaries are designed to map keys to values without preserving any order. This characteristic is beneficial for data where the relationship between keys and values is more important than the sequence of items. It simplifies tasks like looking up specific information without needing to know an element's position, making dictionaries a powerful tool for organising and retrieving data based on meaningful associations rather than order.\n",
    "\n",
    "What if we wanted to add a new key-value pair to the dictionary? This is a simple task in Python:\n",
    "\n"
   ],
   "id": "13a8a42561df704b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Adding a new key-value pair\n",
    "my_dict[\"occupation\"] = \"Engineer\"\n",
    "\n",
    "# Inspecting the updated dictionary\n",
    "print(my_dict)"
   ],
   "id": "4f9ccd4eaffc980",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Great, but let's now modify the value of an existing key. To do this, we simply reassign the value to the key:",
   "id": "d3ecfb3c68e6e55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Modifying a value\n",
    "my_dict[\"age\"] = 31\n",
    "\n",
    "# Inspecting the updated dictionary\n",
    "print(my_dict)"
   ],
   "id": "ed443beac30826a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As you can see, Alice is now 31 years old. This demonstrates how dictionaries are mutable, allowing you to change their contents as needed. Let's try one more change and remove a key-value pair:",
   "id": "aec04816d83fe944"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Removing a key-value pair\n",
    "del my_dict[\"city\"]\n",
    "\n",
    "# Inspecting the updated dictionary\n",
    "print(my_dict)"
   ],
   "id": "f17b35e289cf5258",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dictionary keys are unique and immutable, meaning they can't be changed once assigned. However, the values associated with these keys can be modified, added, or removed as needed. You will find Dictionaries are great for storing data that requires meaningful associations between keys and values, such as user profiles, product details, or any structured data that benefits from key-based retrieval.",
   "id": "9b6e8c65be573bff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Strings",
   "id": "fa6e524bc23dd065"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We've previously encountered strings, which allow you to incorporate text (such as words or file paths) into your code that Python wouldn't inherently understand. Strings are incredibly versatile, capable of containing any character. When dealing with a data file comprising various data types, Python typically interprets it all as strings, leaving the conversion to more specific types, like integers or floats, up to you. Importantly, strings are iterable, meaning they can be indexed character by character, similar to lists.\n",
   "id": "879f8235371fcb43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Defining a string\n",
    "my_string = \"Hello, world!\"\n",
    "\n",
    "# Accessing characters in the string\n",
    "first_char = my_string[0]  # 'H'\n",
    "last_char = my_string[-1]  # '!'\n",
    "\n",
    "# Printing the characters\n",
    "print(f\"First character: {first_char}\")\n",
    "print(f\"Last character: {last_char}\")"
   ],
   "id": "2087d311ab1a438c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This snippet illustrates several key aspects of working with strings:\n",
    "\n",
    "- We create a simple string `my_string` with the value `\"Hello, world!\"` to show how text is stored.\n",
    "- Using our knowledge of indexing, we can extract specific characters from the string. `my_string[0]` retrieves the first character (`'H'`), while `my_string[-1]` fetches the last character (`'!'`), demonstrating how strings are iterable.\n",
    "- The `print()` function displays the first and last characters. The \"f\" before the string literals indicates an **f-string**, which allows for the direct insertion of expressions into string literals using curly braces `{}`. This method simplifies the process of combining text and variables/data in output.\n",
    "\n",
    "Let's see what else we can do with strings. For instance, we can concatenate strings, which means combining them into a single string. This is achieved using the `+` operator:\n"
   ],
   "id": "64f2a66f1b2c546"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Concatenating strings\n",
    "new_string = my_string + \" from Python\"\n",
    "print(new_string)"
   ],
   "id": "e4588b1f320a1a4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here, we've combined the original string `\"Hello, world!\"` with the phrase `\" from Python\"`, creating a new string that reads `\"Hello, world! from Python\"`. This operation is known as string concatenation. Let's now try to replace a portion of the string with another string:",
   "id": "53ad5cf12d735d7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Replacing a substring\n",
    "replaced_string = my_string.replace(\"world\", \"Python\")\n",
    "print(replaced_string)"
   ],
   "id": "fc265a873c280a1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here, we've replaced the substring `\"world\"` in the original string with `\"Python\"`, resulting in the new string `\"Hello, Python!\"`. This operation is known as string replacement. Next, let's try splitting a string into a list of substrings:",
   "id": "c1a3f7a404353888"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Splitting a string\n",
    "split_string = my_string.split(\",\")\n",
    "print(split_string)"
   ],
   "id": "57de7b61254ad2f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here, we've split the original string `\"Hello, world!\"` into a list of substrings, using the comma `\",\"` as the separator. The result is a list `['Hello', ' world!']`, where the comma has been removed. This operation is known as string splitting. Finally, let's try case conversion on a string:",
   "id": "fdd086a8a0671336"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Converting case\n",
    "upper_case = my_string.upper()\n",
    "lower_case = my_string.lower()\n",
    "print(upper_case)\n",
    "print(lower_case)"
   ],
   "id": "974400d1cba48013",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here, we've converted the original string `\"Hello, world!\"` to uppercase using the `upper()` method, resulting in `\"HELLO, WORLD!\"`. We've also converted the string to lowercase using the `lower()` method, resulting in `\"hello, world!\"`. These operations are known as case conversion. \n",
    "\n",
    "Strings are incredibly versatile, offering a wide range of methods for manipulating and extracting data. This flexibility makes them invaluable for working with textual data, such as user input, file contents, or any text-based information you encounter in your code."
   ],
   "id": "5854ef96ae9ed82e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tuples",
   "id": "9bb80412cec32697"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tuples are similar to lists, but they are immutable, meaning their contents cannot be changed after creation. This characteristic makes them useful for storing data that shouldn't be altered, such as a set of coordinates or a date. To understand, let's first inspect our list and attempt to modify it:",
   "id": "f6874dadd1ad29b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(my_list)",
   "id": "1eaff68bf32a59f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's modify the second element of the list:",
   "id": "b4f36ffdbafc6aa3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "my_list[1] = \"new_string\"\n",
    "print(my_list)"
   ],
   "id": "be37b62a1ba1906b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's try the same with a tuple:",
   "id": "f37616a038c65373"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "my_tuple = (1, \"a_string\", True, 3.14, [2, 4, 6])\n",
    "print(my_tuple)"
   ],
   "id": "f6636829bbfbd90e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note a tuple is created using parentheses `()` instead of square brackets `[]`. Now, let's try to modify the second element of the tuple:",
   "id": "377e8fc4aa7215ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "my_tuple[1] = \"new_string\"",
   "id": "231df46def4a0cda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Notice the error message that appears when you try to modify the tuple. This is because tuples are immutable, meaning their contents cannot be changed after creation. This characteristic makes them useful for storing data that shouldn't be altered, such as a set of coordinates or a date.",
   "id": "c1f7bee3c8b8c0aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Functions\n",
    "\n",
    "Functions allow you to encapsulate code into reusable blocks, making your code cleaner and more modular. You can define a function with a name and parameters, and it can return a result.\n",
    "\n",
    "#### Defining a Function\n",
    "\n",
    "To define a function in Python, you use the `def` keyword followed by the function name, parameters, and a colon. The indented block below the function header contains the code that the function will execute.\n"
   ],
   "id": "4a85fb9e58561df2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def greet(name):\n",
    "    print(f\"Hello, {name}!\")"
   ],
   "id": "a40f82e72127c08b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this example, the function `greet()` takes one argument `name` and prints a greeting.\n",
    "\n",
    "#### Calling a Function\n",
    "\n",
    "Once the function is defined, you can call it with the appropriate arguments:"
   ],
   "id": "18ebe3d934ad2961"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "greet(\"Alice\")",
   "id": "79244ca799ddf123",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Functions with Return Values\n",
    "\n",
    "You can also define functions that return values. The `return` statement sends a result back to the caller."
   ],
   "id": "c906914bda7535ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "result = add(5, 3)\n",
    "print(result)"
   ],
   "id": "b92390ded80d25ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this example, the `add()` function takes two arguments and returns their sum.",
   "id": "fb09622965da198a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Indentation\n",
    "Notice above how the code inside the function is indented. In Python, indentation is crucial for code structure. It is used to define blocks of code, such as the body of a function or loop. Indentation helps Python understand the structure of your code, so be sure to use consistent indentation throughout your programs."
   ],
   "id": "39e8a9c228d48365"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T03:13:44.533755Z",
     "start_time": "2024-12-12T03:13:44.527258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if 5 > 3:\n",
    "    print(\"5 is greater than 3\")"
   ],
   "id": "3fbce56791461dc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 is greater than 3\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this example, the code block inside the `if` statement is indented, indicating that it should be executed only if the condition `5 > 3` is `True`. If the condition is `False`, the indented code block will be skipped. Python will give you an error if you skip indentation:",
   "id": "a34ec55961724085"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T03:13:47.479941Z",
     "start_time": "2024-12-12T03:13:47.475746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if 5 > 3:\n",
    "print(\"5 is greater than 3\")"
   ],
   "id": "4afb4d7bde0a9452",
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1960757576.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"/var/folders/rh/n5bp2xln1wnbtpdl521720mc0000gn/T/ipykernel_68021/1960757576.py\"\u001B[0;36m, line \u001B[0;32m2\u001B[0m\n\u001B[0;31m    print(\"5 is greater than 3\")\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m expected an indented block\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The number of spaces used for indentation is up to you, but it must be consistent throughout your code. The standard practice is to use four spaces for each level of indentation. Most code editors will automatically convert tabs to spaces to ensure consistent indentation.",
   "id": "5d5c97261d4f8588"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loops\n",
    "\n",
    "Loops are used to execute a block of code repeatedly, either for a specified number of times or until a condition is met. The most common types of loops in Python are `for` loops and `while` loops.\n",
    "\n",
    "#### For Loop\n",
    "\n",
    "A `for` loop iterates over a sequence (like a list, string, or range) and executes the code block for each element."
   ],
   "id": "69068233d0491572"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(1, 6):\n",
    "    print(i)"
   ],
   "id": "8aa625d2a6b545ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this example, `range(1, 6)` generates numbers from 1 to 5, and the `for` loop prints each number.",
   "id": "fa9b6957b4f77113"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### While Loop\n",
    "\n",
    "A `while` loop runs as long as a condition is `True`. The condition is checked before each iteration, and the loop continues executing until the condition is no longer satisfied."
   ],
   "id": "e117f04485f77e50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "count = 1\n",
    "while count <= 5:\n",
    "    print(count)\n",
    "    count += 1"
   ],
   "id": "2a31dd0e304c6d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Errors and Exceptions",
   "id": "1ae5f15cfde78c99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There are two distinct types of errors in Python: **syntax errors** and **exceptions**. Syntax errors occur when the code is improperly written, such as a missing colon or parentheses. These errors are detected by Python before the code is executed, preventing the program from running. Let's generate a syntax error by omitting a closing parenthesis:",
   "id": "4e57ecb3c4bda0d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Syntax error\n",
    "print(\"Hello, world!\""
   ],
   "id": "66d56eff6ac99eb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "When you run this cell, a `SyntaxError` will be raised, indicating that a closing parenthesis is missing. This error message is Python's way of informing you that the code is improperly written and needs correction before it can be executed.",
   "id": "f977a18d3b7721e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The other type of error, exceptions, occurs when the code is syntactically correct but encounters an issue during execution. These errors are detected while the code is running, causing the program to halt. Let's generate an exception by attempting to divide by zero:",
   "id": "5945c7883a4d4c7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Attempting to divide by zero\n",
    "result = 5 / 0"
   ],
   "id": "45cdd71c2aebbaa9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "When you run this cell, a `ZeroDivisionError` will be raised, indicating that you can't divide by zero. This error message is Python's way of informing you that the operation you attempted is mathematically impossible. Other possible errors include those listed in the table below.\n",
    "\n",
    "\n",
    "&nbsp;  \n",
    "\n",
    "| Exception                | Description                                                       |\n",
    "|--------------------------|-------------------------------------------------------------------|\n",
    "| ArithmeticError          | Raised when an error occurs in numeric calculations               |\n",
    "| AssertionError           | Raised when an assert statement fails                             |\n",
    "| AttributeError           | Raised when attribute reference or assignment fails               |\n",
    "| Exception                | Base class for all exceptions                                     |\n",
    "| EOFError                 | Raised when the input() method hits an \"end of file\" condition (EOF) |\n",
    "| FloatingPointError       | Raised when a floating point calculation fails                    |\n",
    "| GeneratorExit            | Raised when a generator is closed (with the close() method)       |\n",
    "| ImportError              | Raised when an imported module does not exist                     |\n",
    "| IndentationError         | Raised when indentation is not correct                            |\n",
    "| IndexError               | Raised when an index of a sequence does not exist                 |\n",
    "| KeyError                 | Raised when a key does not exist in a dictionary                  |\n",
    "| KeyboardInterrupt        | Raised when the user presses Ctrl+c, Ctrl+z or Delete             |\n",
    "| LookupError              | Raised when errors raised can't be found                          |\n",
    "| MemoryError              | Raised when a program runs out of memory                          |\n",
    "| NameError                | Raised when a variable does not exist                             |\n",
    "| NotImplementedError      | Raised when an abstract method requires an inherited class to override the method |\n",
    "| OSError                  | Raised when a system related operation causes an error            |\n",
    "| OverflowError            | Raised when the result of a numeric calculation is too large      |\n",
    "| ReferenceError           | Raised when a weak reference object does not exist                |\n",
    "| RuntimeError             | Raised when an error occurs that do not belong to any specific exceptions |\n",
    "| StopIteration            | Raised when the next() method of an iterator has no further values |\n",
    "| SyntaxError              | Raised when a syntax error occurs                                 |\n",
    "| TabError                 | Raised when indentation consists of tabs or spaces                |\n",
    "| SystemError              | Raised when a system error occurs                                 |\n",
    "| SystemExit               | Raised when the sys.exit() function is called                     |\n",
    "| TypeError                | Raised when two different types are combined                      |\n",
    "| UnboundLocalError        | Raised when a local variable is referenced before assignment      |\n",
    "| UnicodeError             | Raised when a unicode problem occurs                              |\n",
    "| UnicodeEncodeError       | Raised when a unicode encoding problem occurs                     |\n",
    "| UnicodeDecodeError       | Raised when a unicode decoding problem occurs                     |\n",
    "| UnicodeTranslateError    | Raised when a unicode translation problem occurs                  |\n",
    "| ValueError               | Raised when there is a wrong value in a specified data type       |\n",
    "| ZeroDivisionError        | Raised when the second operator in a division is zero             |\n",
    "\n",
    "While you are unlikely to encounter all of these errors, it's essential to understand the most common ones and how to interpret their messages. Python's error messages are designed to be informative, helping you identify the issue and its location in your code, making you a more effective programmer.\n",
    "\n",
    "While it is outside the scope of this session, you can write programs to handle exceptions, allowing your code to continue running even when errors occur. This is a crucial aspect of programming, ensuring your code can gracefully handle unexpected issues and continue functioning as intended. Within Jupiter notebooks, these are a little easier to catch as the error will be displayed in the output of the cell."
   ],
   "id": "6e4e7ab635208a02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    Exercise: Integrating Basic Concepts\n",
    "  </h1>\n",
    "</div>"
   ],
   "id": "587004f0d9092e3d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's tackle a comprehensive example that incorporates the elements we've discussed.\n",
    "\n",
    "Imagine you're teaching the introductory Quantum Mechanics unit. Surprisingly, after grading the mid-semester exam, you find many scores lower than expected, despite considering the exam quite fair!\n",
    "\n",
    "To avoid alarming students with their individual scores, you opt to calculate the exam's statistical distribution first, allowing students to see where their score stands in relation to the class average and other statistics.\n",
    "\n",
    "The exam scores (out of 120) are as follows: 100, 68, 40, 78, 81, 65, 39, 118, 46, 78, 9, 37, 43, 87, 54, 29, 95, 87, 111, 65, 43, 53, 47, 16, 98, 82, 58, 5, 49, 67, 60, 76, 16, 111, 65, 61, 73, 63, 115, 72, 76, 48, 75, 101, 45, 46, 82, 57, 17, 88, 90, 53, 32, 28, 50, 91, 93, 7, 63, 88, 55, 37, 67, 0, 79.\n",
    "\n",
    "Begin by placing these numbers into a list named \"scores\". You can copy and paste the scores directly and add the list syntax in a cell below.\n"
   ],
   "id": "e11db7fa40b06394"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "20e095057929e35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Our next step is to calculate the average score. While Python has libraries and functions for streamlined calculations, mastering the manual approach is invaluable at this stage.\n",
    " \n",
    "First, start by summing all values using the `sum()` function on the list. This function takes a list as an argument and returns the sum of all its elements.\n",
    "\n",
    "Next, to calculate an average, you take your sum then divide by the count of those numbers using the `len()` function.\n",
    "\n",
    "With this method, go ahead and define a variable called \"average_score\" in the cell below to compute the average from the scores list."
   ],
   "id": "da001ceabb04c31f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "c25a43c796d0dd87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we've calculated the average score for the exam, let's convert that into a percentage. In the cell below, compute the average score's percentage by dividing it by the total points available on the test and then multiplying by 100. Execute the cell to see a sentence displaying the percentage. Examine the provided line that achieves this to understand how it functions.\n",
   "id": "506a21f8c58980c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Another crucial metric is the standard deviation from the mean. This statistic is especially relevant in educational contexts where grades are determined on a curve. The standard deviation provides insight into the spread of exam scores around this average, indicating the variability of students' performance.\n",
    "\n",
    "The formula for calculating the standard deviation is:\n",
    "$$\n",
    "s = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N} (x_i - \\bar{X})^2}\n",
    "$$\n",
    "\n",
    "In this equation:\n",
    "\n",
    "- $s$ is the standard deviation, indicating how scores deviate, on average, from the mean score.\n",
    "- $\\bar{X}$ stands for the average (mean) score.\n",
    "- $N$ represents the total count of scores, offering a denominator that normalizes the sum of squared deviations.\n",
    "- $x_i$ refers to each individual score in the dataset.\n",
    "- The expression $\\sum_{1}^{N}(x_i - \\bar{X})^2$ calculates the sum of squared differences between each score and the mean, highlighting the collective variance from the average.\n",
    "\n",
    "This formula's numerator squares the deviation of each score from the mean before summing these values, which is then normalised by $N-1$ rather than $N$ to account for sample variance in statistics, providing a more accurate representation of dispersion for samples rather than entire populations.\n"
   ],
   "id": "1af3e8c8f043b043"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To compute the standard deviation, the challenging part is determining the numerator of the fraction. Given the tools we've discussed so far, this can be somewhat complex. Hence, I'll introduce a new concept: Numpy arrays, which stand for numerical Python. We'll dive deeper into Numpy arrays next week. For now, take a look at the example below to grasp why they're instrumental for our calculations:\n",
   "id": "e8f092e4b21ea567"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First, let's try to calculate the denominator of the standard deviation formula. This involves $N-1$.",
   "id": "ab21bc18dd317ece"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(scores-1)",
   "id": "56306de0d2e508b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Okay, so I can't subtract an integer from a list. What if I try NumPy arrays?",
   "id": "2e2701525850a997"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "arr_version = np.array(scores)\n",
    "print(arr_version-1)"
   ],
   "id": "54ce0d5ec27a2fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If you look, you should see that each of those scores is the original score with one subtracted off it. This is the power of NumPy arrays. They allow us to perform operations on entire arrays at once, which is crucial for calculating the standard deviation.\n",
    "\n",
    "It's now your turn to calculate numerator component. In the cell below, fill in the variable I'm calling \"top_frac\" to calculate this quantity:\n",
    "$$\n",
    "\\sum_{i=1}^N (x_i - \\bar{X})^2\n",
    "$$\n",
    "\n",
    "Notice here that you don't have to actually calculate it one by one - if we first compute a single array that represents each score with the mean subtracted off and then that value squared, then we finish off top_frac just by summing up that array as we've done before. Feel free to use my variable \"arr_version\"."
   ],
   "id": "55df837c4a0286a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "a796d10275518f66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With that done, we can easily apply the formula to get the final STD - **Hint:** the function np.sqrt() will be useful here.",
   "id": "42a8582677af69c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "9d0a5ad925dd58b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Great! If all steps were followed correctly, you'd discover the average score is 62/120, and the standard deviation is 28. Let's now spoil everything and I will show you how you could have done this with one line:\n",
   "id": "b9961a2ef5204201"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "STD_scores_2 = np.std(arr_version, ddof=1)\n",
    "print(STD_scores_2)"
   ],
   "id": "89d70e2774dc8932",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "What is this `ddof` in NumPy?\n",
    "\n",
    "- Setting `ddof=0` (the default) instructs NumPy to divide by `N`, which is suitable for calculating the population variance and standard deviation.\n",
    "- Setting `ddof=1` modifies the calculation to divide by `N-1`, making it suitable for sample variance and standard deviation. \n",
    "\n",
    "This distinction is crucial in statistics, ensuring that the sample standard deviation accurately reflects the variability of the data. For our purposes, we're working with a sample of exam scores, so we set `ddof=1` to calculate the standard deviation correctly. We will invesigate statistics further in a later practical.\n"
   ],
   "id": "4f77336b993fb3d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Just for a bit of fun, let's create an informative plot to visually represent the students' scores. Don't stress about understanding the plotting details for now — we'll explore this later.",
   "id": "5e1188f0804b0f91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_errorbars(arg, **kws):\n",
    "    plt.hist(scores,20,density=True)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    f, axs = plt.subplots(2, figsize=(7, 2), sharex=True)\n",
    "    sns.pointplot(x=scores, errorbar=arg, **kws, capsize=.3, ax=axs[0])\n",
    "    sns.stripplot(x=scores, jitter=.3, ax=axs[1])\n",
    "    \n",
    "plot_errorbars(\"sd\")"
   ],
   "id": "9705c2700870024d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This plot is a combination of a simplified histogram, a point plot, and a strip plot. Here's what each part represents:\n",
    "\n",
    "Histogram\n",
    "- The top plot is a **Histogram** that displays the frequency distribution of your exam scores.\n",
    "- Each bar represents a range of scores, and the height shows how many students scored within that range.\n",
    "- This visual helps us understand common score ranges and identify where most students fall.\n",
    "\n",
    "Point and Strip Plots\n",
    "- Below the histogram, the **Point Plot** indicates the average (mean) score with a horizontal line for the standard deviation.\n",
    "- The mean score, represented by the dot, is around 62. The line extending from it shows the standard deviation, which is about 28.3 points.\n",
    "- This means most scores are within 28.3 points above or below the average, giving you a sense of how spread out the scores are.\n",
    "- The **Strip Plot** displays each student's individual score, giving us a look at each unique score without them stacking on top of each other.\n",
    "\n",
    "What This Means for the Students\n",
    "- If their score is close to the average (62), they're in the most common score range for this exam.\n",
    "- If their score is far from the mean, the standard deviation helps them understand how their score compares with the rest of the class.\n",
    "- Remember, this distribution is a snapshot of this particular exam performance and each exam can have a different pattern.\n",
    "\n",
    "What This Means for you, the Teacher\n",
    "- The histogram helps you understand the distribution of scores, identifying common score ranges and outliers.\n",
    "- Perhaps you set the exam too difficult here and you may need to adjust in the future!\n"
   ],
   "id": "72087a5aaba43429"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    End of Exercise\n",
    "  </h1>\n",
    "</div>"
   ],
   "id": "cbdd7440ea765792"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Importing Data\n",
    "\n",
    "Computer-based data storage has evolved significantly from the days when nearly all information was kept in plaintext ASCII files, which are essentially text tables (American Standard Code for Information Interchange). While data storage practices have modernised, text tables continue to be a highly efficient means of storing data due to their simplicity and ease of access. Below, we will focus on the \"quantum_mechanics.grades\" file, an example of a text table file. It is common for such data files to have custom extensions like `.grades` or `.students` instead of the generic `.txt` to indicate their content type.\n",
    "\n",
    "Please ensure that the \"quantum_mechanics.grades\" file is in the same directory as this notebook. We will then use Pandas, a powerful Python library for data manipulation and analysis, to load this data into a DataFrame. This step will help us understand the structure and contents of the data. If you have not yet installed Pandas, or need the latest version, make sure to install or update Pandas before we begin.\n",
    "\n",
    "First, let's check whether you already have Pandas installed and its current version. You can do this by running the following code in a Jupyter notebook cell:"
   ],
   "id": "f679bc4b42884a32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ],
   "id": "c73a99cc085f2ccc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If Pandas is not installed on your system, you can install it using `pip`, the Python package installer. Execute the following command in your Jupyter notebook:\n",
    "```python\n",
    "!pip install pandas"
   ],
   "id": "adec070d90a10132"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If you already have Pandas installed and need to update it to the latest version, you can do so by executing the following command in your Jupyter notebook:\n",
    "```python\n",
    "!pip install --upgrade pandas"
   ],
   "id": "b901429c6050af92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "When working with data files, it's crucial to understand that they can come in various formats, each suitable for different types of data manipulation and analysis tasks. Two common terms you'll encounter are ASCII and delimited text files:\n",
    "\n",
    "- **ASCII (American Standard Code for Information Interchange)**: This is a character encoding standard used to represent text in computers and other communication devices. Most text files you'll encounter are based on ASCII or one of its extensions, which supports additional characters beyond the basic set.\n",
    "\n",
    "- **Delimited Text Files**: These are text files where data elements are separated by a specific character, known as a delimiter. Common delimiters include commas (`,`) for CSV (Comma-Separated Values) files, tabs for TSV (Tab-Separated Values) files, and spaces for space-delimited files. Delimited files are easy to read and write both by humans and machines, making them widely used in data processing.\n",
    "\n",
    "Before importing data, it's important to determine the type of file you are dealing with. Here are a few methods to help you identify the file format:\n",
    "\n",
    "1. **File Extension Inspection**: The file extension often gives a clue about the format of the data. Common extensions include:\n",
    "   - `.csv` for comma-separated values\n",
    "   - `.tsv` for tab-separated values\n",
    "   - `.txt` for plain text files\n",
    "   - Custom extensions like `.grades`, `.students`, etc., which might need further inspection.\n",
    "\n",
    "2. **Opening the File in a Text Editor**: Open the file in a text editor (e.g., Notepad, Sublime Text, VS Code) and look at the first few lines to identify patterns:\n",
    "   - **Comma-Separated Values (CSV)**: Look for commas separating values.\n",
    "   - **Tab-Separated Values (TSV)**: Look for tabs separating values.\n",
    "   - **Space-Delimited**: Look for spaces separating values.\n",
    "   - **ASCII or Plain Text**: Look for simple text without any special delimiters.\n",
    "\n",
    "3. **Using Python's Built-in Functions**: Read the first few lines of the file using Python to understand its format. For example:\n",
    "   ```python\n",
    "   def inspect_file(file_name):\n",
    "       with open(file_name, 'r') as file:\n",
    "           for i in range (5):  # Read the first 5 lines\n",
    "               line = file.readline().strip()\n",
    "               print(line)\n",
    "\n",
    "Let's inspect our file to understand its structure and contents before importing it into a DataFrame. We will use the above `inspect_file()` function to read the first few lines of the \"quantum_mechanics.grades\" file and identify its format."
   ],
   "id": "1d236c6f7bd3f003"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to inspect the first few lines of a file\n",
    "def inspect_file(file_name):\n",
    "       with open(file_name, 'r') as file:\n",
    "           for i in range (5):  # Read the first 5 lines\n",
    "               line = file.readline().strip()\n",
    "               print(line)\n",
    "\n",
    "# Assign 'quantum_mechanics.grades' to the file_name variable\n",
    "file_name = 'quantum_mechanics.grades'\n",
    "\n",
    "# Run the inspect_file function on the file\n",
    "inspect_file(file_name)"
   ],
   "id": "c7908b8ad60be9c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It appears our file is a simple text table with grades listed in each row. The grades are likely separated by spaces or tabs, as we can see multiple values in each line. We will now import this data into a Pandas DataFrame to explore it further. We can use the `read_csv()` function from Pandas to load the data from the file into a DataFrame. This function is versatile and can handle various file formats, including CSV, TSV, and custom delimited files.\n",
    "\n",
    "For our data, we will assign the separator as `\\s+` to handle multiple spaces as a delimiter. The `\\s+` separator is a regular expression pattern that matches one or more whitespace characters, which includes spaces and tabs. This ensures that any number of spaces between columns is correctly interpreted as a delimiter.\n",
    "\n",
    "Here are some common separators you might encounter:\n",
    "\n",
    "- `,` for comma-separated values (CSV)\n",
    "- `\\t` for tab-separated values (TSV)\n",
    "- `;` for semicolon-separated values (often used in European data sets)\n",
    "- `|` for pipe-separated values\n",
    "\n",
    "To load the \"quantum_mechanics.grades\" file into a DataFrame, we will use Pandas' `read_csv` function with the following parameters:\n",
    "\n",
    "- **`sep`**: Specifies the character that separates values in our data file. For this dataset, grades are separated by spaces, so we use `sep='\\s+'`, a regular expression that matches one or more spaces.\n",
    "- **`header`**: This parameter is set to `None` indicating that the first line in the file is not a header row. It's common for data files without column names. If the data file does contain a header row, you can set `header=0` to use the first row as column headers.\n",
    "- **`names`**: Allows us to assign names to the columns in our DataFrame. Since our data consists of a single column of grades, we will name it 'Grades'. If the file already contains column headers and `header=0` is used, this parameter can be omitted.\n"
   ],
   "id": "d350c26303c51c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "grades_df = pd.read_csv('quantum_mechanics.grades', sep='\\s+', header=None, names=['Name', 'Grades'])",
   "id": "4e30d2a4d80cde26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's take a look at the first few rows of the dataset to ensure it's loaded correctly. We can use the `head()` method to display the first five rows of the DataFrame. This method is useful for quickly examining the structure and contents of the data.\n",
   "id": "af64fb4b29e9ee20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(grades_df.head())",
   "id": "85bccf642a1166ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You may recall from the first practical, we spent some time determining the average grade and standard deviation for the quantum mechanics course. Now that we have the data loaded into a DataFrame, we can use Pandas to calculate these statistics more efficiently. The `describe()` method provides a summary of basic statistics for the data, including the count, mean, standard deviation, minimum, maximum, and quartiles.",
   "id": "1da2c0737a6a64f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(grades_df.describe())",
   "id": "9d440079c6d04c97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The other values here include the minimum and maximum grades, as well as the 25th, 50th, and 75th percentiles. These statistics provide a comprehensive overview of the data distribution, allowing us to understand the range of grades and how they are distributed.",
   "id": "47775883719c7e14"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In addition to `describe()`, here are some other useful methods for inspecting data:\n",
    "\n",
    "- **`info()`**: Provides a concise summary of the DataFrame, including the number of non-null values and data types of each column.\n",
    "- **`shape`**: Returns a tuple representing the dimensionality of the DataFrame.\n",
    "- **`columns`**: Returns the column labels of the DataFrame.\n",
    "- **`dtypes`**: Returns the data types of each column.\n"
   ],
   "id": "1ca357b0abeef177"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's inspect the DataFrame using these methods\n",
    "print(grades_df.info())  # Display a concise summary of the DataFrame\n",
    "print(\"\\nDataFrame Shape:\", grades_df.shape)  # Display the shape of the DataFrame\n",
    "print(\"\\nColumn Labels:\", grades_df.columns)  # Display the column labels\n",
    "print(\"\\nData Types:\\n\", grades_df.dtypes)  # Display the data types of each column"
   ],
   "id": "54d48dd18829d8b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We see there are two columns in the DataFrame: 'Name' and 'Grades'. The 'Name' column contains student names, while the 'Grades' column contains the grades for each student. The data types are `object` for 'Name' and `int64` for 'Grades', indicating that 'Name' is a text column and 'Grades' is a numeric column. We also see the shape of the DataFrame is (65, 2), meaning it has 65 rows and 2 columns.",
   "id": "1571a52a7b703770"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    Exercise: Importing Data and Inspection\n",
    "  </h1>\n",
    "</div>"
   ],
   "id": "38a08e2cc371d5cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this activity, you will work with a dataset titled `carbonmonoxide-qld-2022.csv`, which includes measurements of carbon monoxide levels across Queensland in 2022. Your task is to import this dataset into a DataFrame, inspect it for an initial understanding, and prepare it for further analysis.\n",
    "\n",
    "First, ensure the dataset file is in the same directory as your notebook or accessible via a path. Use Pandas to load the data into a DataFrame. Given that the file is a CSV, ensure you incorporate the correct delimiter.\n"
   ],
   "id": "33ced1bb70cae6fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "e83260de6739d712",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Display the first few rows of the dataset",
   "id": "2668c7f799781d97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "a5d8930fc08f7fb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Display the first 15 rows of the dataset. Hint: the `head()` function allows you to specify the number of rows to display.\n",
   "id": "3c285866acdccc10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "99060b79b16c2f70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Print statistical summaries of the data\n",
   "id": "54ff04ec52203b54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "c8e69cc4ed6d5600",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Provide some code to understand the data file's structure and contents. \n",
   "id": "3dfa8069dfe8452a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "1d0e661bf4fae35d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If you've run the above correctly, you will have printed a concise summary of the DataFrame, the shape of the DataFrame, the column labels, and the data types of each column. Here's a breakdown of the output:\n",
    "\n",
    "1. **Class Type**:\n",
    "   - The output begins by confirming that `co_df` is indeed a DataFrame object, which is a standard data structure in Pandas used for data manipulation.\n",
    "\n",
    "2. **Index**:\n",
    "   - `RangeIndex: 8760 entries, 0 to 8759` indicates that the DataFrame uses a default integer index for row labeling, starting at 0 and ending at 8759. This index includes 8760 entries, suggesting the dataset covers 8760 periods (hourly data over a year, as there are 24*365 = 8760 hours in a non-leap year).\n",
    "\n",
    "3. **Columns**:\n",
    "   - The DataFrame contains four columns. The method lists each column along with the number of non-null entries and the data type (`Dtype`) of each column:\n",
    "     - **Date**: All 8760 entries are non-null, meaning there are no missing values in this column. The data type is `object`, typically used in Pandas for text or mixed numeric and non-numeric data.\n",
    "     - **Time**: Similar to the Date column, this also has 8760 non-null entries and is of type `object`.\n",
    "     - **South Brisbane (South East Queensland) (ppm)**: This column has 8190 non-null entries, indicating some missing data (570 missing values). The data type is `float64`, used for floating-point numbers.\n",
    "     - **Woolloongabba (South East Queensland) (ppm)**: Contains 8438 non-null entries, also suggesting missing data (322 missing values), and it is of type `float64`.\n",
    "\n",
    "4. **Data Types Summary**:\n",
    "   - The DataFrame has two data types: `float64` for numeric floating-point data and `object` for text or mixed data types. The summary shows there are two columns of each type.\n",
    "\n",
    "5. **Memory Usage**:\n",
    "   - The memory usage of this DataFrame is approximately 273.9 KB. This information is useful for understanding how much memory the DataFrame consumes, which can be important when working with large datasets or when performance is a concern.\n",
    "\n",
    "By understanding each of these aspects, you can better assess the initial state of your data and plan any necessary preprocessing steps, such as data cleaning and data transformation. \n"
   ],
   "id": "f464c85643787dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    End of Exercise\n",
    "  </h1>"
   ],
   "id": "1518065f13614d1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Data cleaning is a critical step in the data analysis process, involving the refinement of raw data into a more usable format. This stage is essential for ensuring the accuracy and quality of insights derived from data analysis. Here are key aspects and benefits of data cleaning:\n",
    "\n",
    "- Clean data is crucial for generating reliable and accurate analysis results. Inaccurate data can lead to faulty conclusions and decisions.\n",
    "- Cleaning data helps in reducing redundancy and improving the efficiency of data storage and data processing.\n",
    "- Well-organised and clean data is easier to manipulate and analyse, making the data analysis process smoother and faster.\n",
    "\n",
    "Common data cleaning tasks include:\n",
    "- Identifying and addressing missing data, which may involve imputation (filling missing values) or removal, depending on the scenario.\n",
    "- Eliminating duplicate records that may skew analysis results.\n",
    "- Ensuring consistency in data types and formats across the dataset, which is crucial for comparative analysis.\n",
    "\n",
    "Before you begin cleaning data, it's important to understand the context and goals of your analysis, as these factors heavily influence how you approach data cleaning. For instance, the way we prepare data for forecasting future trends (predictive modeling) might differ from how we set it up just to describe what has happened (descriptive analysis).\n",
    "\n",
    "In the following sections, we will apply some data cleaning techniques to our carbon monoxide data, allowing you to practice and observe their impact on data quality and analysis outcomes."
   ],
   "id": "b54e537d97859646"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Handling Missing Values\n",
    "\n",
    "Recall we used the `info()` method to identify missing values in the carbon monoxide dataset. Let's revisit this information and explore the missing values in more detail. Missing values can occur for various reasons, such as data collection errors, sensor malfunctions, or data processing issues. It's essential to identify and address missing values appropriately to ensure the accuracy and reliability of data analysis results."
   ],
   "id": "94824368ab5b129d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "missing_values_count = co_df.isnull().sum() # Count the number of missing values in each column\n",
    "print(\"Missing Values per Column:\")\n",
    "print(missing_values_count)"
   ],
   "id": "9e89e441f633bd6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The output shows the number of missing values in each column of the dataset. The `isnull()` method returns a DataFrame of the same shape as `co_df`, where each cell contains a boolean value indicating whether the corresponding cell in `co_df` is null (`True`) or not (`False`). By summing these boolean values, we can count the missing values in each column.",
   "id": "c0db28b299092a49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To visualise the missing values more clearly, we can use a heatmap to represent the distribution of missing values across the dataset. This visualisation helps us identify patterns and clusters of missing values, making it easier to decide on the best approach for handling them. Given visualising data is covered later, do not concern yourself with the code below, but rather focus on the concept of visualising missing values.",
   "id": "a30f20abde8e0cc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns # Import Seaborn for data visualisation\n",
    "import matplotlib.pyplot as plt # Import Matplotlib for plotting\n",
    "\n",
    "# Visualise missing values using a heatmap\n",
    "plt.figure(figsize=(10, 6)) # Set the figure size\n",
    "sns.heatmap(co_df.isnull(), cbar=False, cmap='Reds') # Create a heatmap of missing values\n",
    "plt.title('Heatmap of Missing Values') # Set the plot title\n",
    "plt.show() # Display the plot"
   ],
   "id": "c43f6facab17cdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The output shows a heatmap of missing values in the dataset, with missing values represented in red. The heatmap provides a visual representation of the distribution of missing values across the dataset, highlighting columns with a high concentration of missing values. This visualisation can help us identify patterns and clusters of missing values, guiding our decision on how to handle them effectively.",
   "id": "d535f4b75ff4194a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's say we want to address the missing values in the dataset by filling them with the mean value of each column. This process, known as imputation, is a common technique for handling missing data. We can use the `fillna()` method to replace missing values with the mean of each column. This method is flexible and allows us to specify the value used for filling missing entries. To start with, we need to import the NumPy library, which provides support for mathematical operations in Python. We will then define the mean value for each column and fill the missing values accordingly. Finally, we will verify that the dataset no longer contains any missing values.",
   "id": "3ba68797e8b1a742"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np # Import NumPy for numerical operations\n",
    "numeric_means = co_df.select_dtypes(include=[np.number]).mean() # Calculate the mean value for each numeric column\n",
    "co_df.fillna(numeric_means, inplace=True) # Fill missing values with the mean value for each column"
   ],
   "id": "bde506e44ec9b4a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The code above calculates the mean value for each numeric column in the dataset using the `mean()` method. It then fills the missing values in each numeric column with the corresponding mean value using the `fillna()` method. The `inplace=True` parameter ensures that the changes are applied directly to the DataFrame, updating the dataset with the imputed values.",
   "id": "b4bf60a5bda61d2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(co_df.isnull().sum())",
   "id": "44e799b45a6e8978",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that using the mean to fill missing values is just one of many imputation strategies. Depending on the context and nature of the data, you may choose other methods, such as filling with the median, mode, or a specific value based on domain knowledge. It's essential to consider the implications of each imputation strategy on the data quality and analysis outcomes.",
   "id": "1a1d5621328c182a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's again visualise the missing values using a heatmap to confirm that the dataset no longer contains any missing values.\n",
   "id": "ced8df52fe6f506b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 6)) # Set the figure size\n",
    "sns.heatmap(co_df.isnull(), cbar=False, cmap='Reds') # Create a heatmap of missing values\n",
    "plt.title('Heatmap of Missing Values') # Set the plot title\n",
    "plt.show() # Display the plot"
   ],
   "id": "88686706d0276eaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The heatmap now shows no missing values in the dataset, as all cells are white, indicating that the missing values have been successfully imputed. This visualisation confirms that the dataset is now complete, with no missing values present. Imputation is a common technique for handling missing data and ensuring the completeness and reliability of the dataset for analysis.",
   "id": "7e5af6e110bed041"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Another common data cleaning task is to remove rows with missing values entirely. This approach is useful when the missing values are significant in number or when imputation is not appropriate for the analysis. We can use the `dropna()` method to eliminate rows with missing values from the DataFrame. This method removes any row containing at least one missing value, effectively reducing the dataset size.\n",
    "\n",
    "First, we have to reload the original dataset to restore the missing values before proceeding with the removal of rows with missing values."
   ],
   "id": "915a3483bccb49d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "co_df = pd.read_csv('carbonmonoxide-qld-2022.csv', sep=',') # Reload the original dataset",
   "id": "775927bf712576b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we can use the `dropna()` method to remove rows with missing values from the DataFrame.",
   "id": "4847b936c747d399"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "co_df.dropna(inplace=True)",
   "id": "f61bcb27de8121ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After removing rows with missing values, we can verify that the dataset no longer contains any missing values by checking the sum of missing values per column.\n",
   "id": "7ba49f792ee77c7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(co_df.isnull().sum())",
   "id": "62d17a41a51d4c05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Notice that the output shows zero missing values for all columns, indicating that the dataset no longer contains any missing values. Removing rows with missing values can be an effective strategy for improving data quality and analysis outcomes, especially when the missing values are significant in number or when imputation is not suitable. If we now check the dataset's information, we should see that all columns have the same number of non-null entries.",
   "id": "87d1694eb14d75e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(co_df.info())",
   "id": "2e4a6e45a1b69aec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "However, notice that the number of entries has decreased. This reduction in the number of rows is due to the removal of rows with missing values. It's essential to consider the trade-offs between data quality and data quantity when deciding whether to remove rows with missing values. In some cases, the loss of data may outweigh the benefits of removing missing values, so it's crucial to evaluate the impact on the analysis outcomes. In astrophyics, for example, removing rows with missing values may not be an option due to the scarcity of data points!\n",
   "id": "c9178f9cc4170512"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Removing Duplicates\n",
    "Another common data cleaning task is to identify and remove duplicate records from the dataset. Duplicate records can skew analysis results and lead to inaccurate conclusions. We can use the `duplicated()` method to identify duplicate rows in the DataFrame. This method returns a boolean Series indicating whether each row is a duplicate of a previous row.\n"
   ],
   "id": "afa85127c1bb2c6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "co_df = pd.read_csv('carbonmonoxide-qld-2022.csv', sep=',') # Reload the original dataset to restore the duplicate records\n",
    "duplicates = co_df.duplicated(subset=['Date', 'Time'], keep=False) # Check for duplicate records based on the 'Date' and 'Time' columns\n",
    "print(\"Number of duplicates: \", duplicates.sum())"
   ],
   "id": "182c0c8b7f3feae4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this case, there are no duplicate records in the dataset. Let's instead contaiminate the dataset with duplicates and then remove them. First, let's duplicate the first two rows of the dataset, add them to the end of the DataFrame, and then shuffle the DataFrame to ensure the duplicates are not just at the bottom.\n",
   "id": "ff99abb5d1d5159e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "duplicates_to_add = co_df.iloc[0:2] # Select the first two rows\n",
    "co_df_with_duplicates = pd.concat([co_df, duplicates_to_add], ignore_index=True) # Add the duplicates to the end of the DataFrame\n",
    "co_df_with_duplicates = co_df_with_duplicates.sample(frac=1, random_state=1).reset_index(drop=True) # Shuffle the DataFrame"
   ],
   "id": "259357af8f5e92e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's check the number of rows in the original dataset and the contaminated dataset to confirm that the duplicates have been added.\n",
   "id": "486498edb6d21dee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nNumber of rows in the original data frame:\", len(co_df))\n",
    "print(\"\\nNumber of rows in our contaminated data frame:\", len(co_df_with_duplicates))"
   ],
   "id": "7713d85d0f670892",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can re-run the code from earlier to identify the number of duplicate records in the contaminated dataset.\n",
   "id": "1556a9e6b96e7a6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "duplicates = co_df_with_duplicates.duplicated(subset=['Date', 'Time'], keep=False)\n",
    "print(\"Number of duplicates: \", duplicates.sum())"
   ],
   "id": "5860905b4b63885f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can also display the duplicate rows to see which records are duplicated.\n",
   "id": "8f84dd96db846b35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_duplicate_rows = co_df_with_duplicates[co_df_with_duplicates.duplicated(keep=False)]\n",
    "all_duplicate_rows"
   ],
   "id": "b541e917642760bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As you can see, the DataFrame now contains duplicate records. We can use the `drop_duplicates()` method to remove these duplicate rows from the DataFrame. This method removes duplicate rows based on the specified columns, keeping only the first occurrence of each unique row. \n",
   "id": "5fdfa910f180d3d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "co_df_with_duplicates.drop_duplicates(subset=['Date', 'Time'], keep='first', inplace=True)",
   "id": "c393e23ef8ae4a60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The `drop_duplicates()` method removes duplicate rows from the DataFrame based on the specified columns. The `keep='first'` parameter ensures that only the first occurrence of each unique row is retained, while subsequent duplicates are removed. After removing the duplicate records, we can verify that the dataset no longer contains any duplicate rows by running our checks from above:\n",
   "id": "bc527f497b7f6d8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "duplicates = co_df_with_duplicates.duplicated(subset=['Date', 'Time'], keep=False)\n",
    "print(\"Number of duplicates: \", duplicates.sum())\n",
    "all_duplicate_rows = co_df_with_duplicates[co_df_with_duplicates.duplicated(keep=False)]\n",
    "all_duplicate_rows"
   ],
   "id": "2c0d1edf5c8ac2cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The output confirms that the dataset no longer contains any duplicate records, as the number of duplicates is now zero. Removing duplicate records is essential for ensuring the accuracy and reliability of data analysis results, as duplicate records can skew analysis outcomes and lead to incorrect conclusions. Say we wanted to now save our cleansed dataset to a new file. We can use the `to_csv()` method to write the DataFrame to a CSV file. This method allows us to specify the file path and name for the output file.\n",
   "id": "51398a36f7b60ddd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "co_df_with_duplicates.to_csv('carbonmonoxide-qld-2022-cleaned.csv', index=False)",
   "id": "e5ec93688fffdefd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that since we introduced duplicates into the dataset, the saved the cleansed dataset `carbonmonoxide-qld-2022-cleaned.csv` should be exactly the same as our original dataset `carbonmonoxide-qld-2022.csv`! In a real-world scenario, you would save the cleansed dataset to a new file to preserve the original data for reference and comparison.\n",
   "id": "90b6c8f0f6e3d820"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Standardising Data Formats\n",
    "\n",
    "Standardising data formats is crucial for ensuring consistency and comparability across the dataset. Inconsistent data formats can lead to errors in data analysis and visualisation, making it challenging to draw meaningful insights from the data. Let's explore an example of standardising data formats by converting the 'Date' and 'Time' columns in the carbon monoxide dataset to a single datetime column.\n",
    "\n",
    "First, let's reload the original dataset to restore the 'Date' and 'Time' columns."
   ],
   "id": "bf7703849d7ec22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "co_df = pd.read_csv('carbonmonoxide-qld-2022.csv', sep=',') # Reload the original dataset\n",
    "print(co_df[['Date', 'Time']].head()) # Display the 'Date' and 'Time' columns"
   ],
   "id": "5246386cfbd636c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The 'Date' and 'Time' columns are currently stored as separate columns in the dataset. We can combine these columns into a single datetime column by concatenating the 'Date' and 'Time' values and converting them to a datetime format. This process allows us to standardise the data format and create a unified timestamp for each record.\n",
   "id": "5454a3cbe9b0b0a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 1: Convert 'Date' to datetime type without a time component",
   "id": "22e5bc4ffff63f34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "co_df['Date'] = pd.to_datetime(co_df['Date'], format='%d/%m/%Y') # Convert 'Date' to datetime format",
   "id": "9cdae4f4f4ce63f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the above, we use the `pd.to_datetime()` method to convert the 'Date' column to a datetime format. The `format='%d/%m/%Y'` parameter specifies the date format in the 'Date' column, which is 'day/month/year'. This format is common in many countries, including Australia, where the day precedes the month in date representations.",
   "id": "963f7c12d3c39073"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 2: Ensure 'Time' is in proper string format if it's not already a timedelta (e.g., '1:00' should be '01:00:00' to match HH:MM:SS)",
   "id": "e3c874b6f0c329f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "co_df['Time'] = pd.to_timedelta(co_df['Time'] + ':00')",
   "id": "4e28ee28ac6ab469",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the above, we use the `pd.to_timedelta()` method to convert the 'Time' column to a timedelta format. We append ':00' to the 'Time' values to ensure they are in the proper HH:MM:SS format. This step is crucial for standardising the time component and ensuring consistency in the datetime representation.\n",
   "id": "705109daa3433685"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 3: Combine 'Date' and 'Time' into a new 'DateTime' column",
   "id": "c5f87329a021ea80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "co_df['DateTime'] = co_df['Date'] + co_df['Time']",
   "id": "6a8c75bfb9ddcd0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finaly, in the above, we combine the 'Date' and 'Time' columns into a new 'DateTime' column by adding them together. This operation creates a unified datetime column that includes both the date and time components for each record. The resulting 'DateTime' column standardises the data format and provides a consistent timestamp for analysis.",
   "id": "dd5a95a7dcd23f3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's now display the first few rows to verify the changes",
   "id": "8fc14204b92e8da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(co_df[['Date', 'Time', 'DateTime']].head())",
   "id": "c19feb542874a9dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since we no longer need the 'Date' and 'Time' columns, we can drop them from the dataset to avoid redundancy. We can use the `drop()` method to remove these columns from the DataFrame.",
   "id": "12367e09dad5e4e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "co_df.drop(columns=['Date', 'Time'], inplace=True)",
   "id": "da104ed9c83061bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's inspect the DataFrame to ensure the 'Date' and 'Time' columns have been successfully removed.",
   "id": "93429f32575610ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "co_df",
   "id": "c2df404c0c4eb5ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The output confirms that the 'Date' and 'Time' columns have been removed from the dataset, leaving only the 'DateTime' column as the unified timestamp for each record. \n",
    "\n",
    "While not critical, perhaps you'd like to move the 'DateTime' column to the front of the DataFrame for better visibility. We can achieve this by reordering the columns in the DataFrame."
   ],
   "id": "f77b623c06a271c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "co_df = co_df[['DateTime'] + [col for col in co_df.columns if col != 'DateTime']] # Move 'DateTime' column to the front",
   "id": "b2f3f3565a5c9e35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the above, we use a list comprehension to reorder the columns in the DataFrame. The expression `[col for col in co_df.columns if col != 'DateTime']` iterates over the columns in the DataFrame and excludes the 'DateTime' column. We then concatenate the 'DateTime' column with the remaining columns to move it to the front of the DataFrame. Let's now inspect the DataFrame to confirm the column reordering.\n",
   "id": "10e2847a58beb631"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "co_df",
   "id": "f8077d540e06d60b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The code above reorders the columns in the DataFrame, moving the 'DateTime' column to the front. This step enhances the readability of the dataset and ensures that the primary timestamp information is easily accessible for analysis.\n",
    "\n",
    "The above is just one example of standardising data formats in a dataset. Depending on the context and requirements of your analysis, you may need to perform additional data standardisation tasks, such as converting units of measurement, normalising data ranges, or transforming categorical variables into numerical representations. These steps are essential for ensuring data consistency and comparability across the dataset, enabling more accurate and reliable analysis outcomes."
   ],
   "id": "7a4b0e3a5dc25bf8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    Worked Example: Astronomical Data Transformation\n",
    "  </h1>\n",
    "</div>"
   ],
   "id": "d1682bf5eef249e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In astronomy, accurate coordinate systems are crucial for locating objects in the sky. However, these coordinates can change over time due to the precession of Earth's axis. Therefore, astronomers often need to convert coordinates from one epoch to another to maintain accuracy in their observations and comparisons.\n",
    "\n",
    "In this example, we will explore the process of converting astronomical coordinates from the J1950 epoch to the J2000 epoch. We will start by importing a dataset containing coordinates in the J1950 epoch, and then we will apply a transformation to update these coordinates to the J2000 standard.\n",
    "\n",
    "This transformation is essential for ensuring that the coordinates remain relevant and accurate for current astronomical research and applications."
   ],
   "id": "4bab6d30ebbd0dc2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1. Importing and Inspecting the Data\n",
    "\n",
    "Let's start by importing the data and taking a look at the first few rows."
   ],
   "id": "24d2f2e33b42c710"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd # Importing pandas library\n",
    "df_J1950 = pd.read_csv('Astronomical_Coordinates_J1950.csv') # Importing the dataset\n",
    "print(df_J1950.head(10)) # Displaying the first 10 rows of the dataset"
   ],
   "id": "f13302024ddccae2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The dataset contains two columns: `RA_J1950` and `Dec_J1950`, representing the right ascension and declination coordinates in the J1950 epoch, respectively. These coordinates are essential for locating celestial objects in the sky. Let's proceed with transforming these coordinates to the J2000 epoch.\n",
   "id": "732d56d84832d85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from astropy.coordinates import SkyCoord # Importing the SkyCoord class from the astropy.coordinates module\n",
    "import astropy.units as u # Importing the astropy.units module"
   ],
   "id": "37dad78e643a438e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The above code imports the necessary modules for performing the coordinate transformation. We will use the `SkyCoord` class from the `astropy.coordinates` module to represent the astronomical coordinates. The `astropy.units` module will help us define the units for the coordinates.\n",
   "id": "9764a57c6479bc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "coords_J1950 = SkyCoord(ra=df_J1950['RA (J1950)'], dec=df_J1950['DEC (J1950)'],\n",
    "                        unit=(u.hourangle, u.deg), frame='fk4', obstime='J1950') # Creating a SkyCoord object with J1950 coordinates"
   ],
   "id": "11b97807f51d66a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The above code creates a `SkyCoord` object named `coords_J1950` with the right ascension and declination coordinates from the dataset. We specify the units for the right ascension (hourangle) and declination (degrees) and set the frame to 'fk4' to indicate the J1950 epoch. The `obstime` parameter is set to 'J1950' to specify the epoch of the coordinates.\n",
   "id": "76acecb12bed364d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "coords_J2000 = coords_J1950.transform_to('fk5') # Transforming the coordinates to J2000",
   "id": "80ac49e149205405",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The `transform_to` method is used to convert the coordinates from the J1950 epoch to the J2000 epoch. The method takes the target frame 'fk5' as an argument, which represents the J2000 epoch. The transformed coordinates are stored in the `coords_J2000` object. While this example is specific to astrophysical data, the concept of transforming data from one format to another is applicable to various fields of study. It may be as simple as converting units or as complex as changing coordinate systems, as demonstrated here.\n",
   "id": "c2cf81d897829a43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's print the original and transformed coordinates side by side to compare the differences.",
   "id": "50c737e5864369e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print original and transformed coordinates side by side\n",
    "for original, transformed in zip(coords_J1950, coords_J2000):\n",
    "    print(f\"J1950: RA {original.ra.to_string(unit=u.hour, sep=':', pad=True)}, DEC {original.dec.to_string(unit=u.deg, sep=':', alwayssign=True, pad=True)}\")\n",
    "    transformed_ra_str = transformed.ra.to_string(unit=u.hour, sep=':', pad=True, precision=0)\n",
    "    transformed_dec_str = transformed.dec.to_string(unit=u.deg, sep=':', alwayssign=True, pad=True, precision=0)\n",
    "    print(f\"J2000: RA {transformed_ra_str}, DEC {transformed_dec_str}\")\n",
    "    print()  # Adds a blank line for better readability between entries\n"
   ],
   "id": "e17322a112a1d870",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can also visualize the differences between the J1950 and J2000 coordinates using a plot with vector arrows to represent the changes in the coordinates.\n",
   "id": "da44f86fb26106f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Extract RA and DEC for both epochs\n",
    "ra_J1950 = coords_J1950.ra.wrap_at(180*u.deg).degree\n",
    "dec_J1950 = coords_J1950.dec.degree\n",
    "ra_J2000 = coords_J2000.ra.wrap_at(180*u.deg).degree\n",
    "dec_J2000 = coords_J2000.dec.degree\n",
    "\n",
    "# Calculate differences for vector arrows\n",
    "delta_ra = ra_J2000 - ra_J1950\n",
    "delta_dec = dec_J2000 - dec_J1950\n",
    "\n",
    "# Plot J1950 coordinates with star markers\n",
    "ax.scatter(ra_J1950, dec_J1950, color='yellow', alpha=0.6, edgecolors='none', s=200, marker='*', label='J1950')\n",
    "# Plot J2000 coordinates with star markers\n",
    "ax.scatter(ra_J2000, dec_J2000, color='red', alpha=0.6, edgecolors='none', s=200, marker='*', label='J2000')\n",
    "\n",
    "# Adding vector arrows\n",
    "ax.quiver(ra_J1950, dec_J1950, delta_ra, delta_dec, angles='xy', scale_units='xy', scale=1, color='green', width=0.003, headwidth=8, headlength=4)\n",
    "\n",
    "# Setting the limits for the plot to zoom in on the region of interest\n",
    "ax.set_xlabel('Right Ascension (degrees)')\n",
    "ax.set_ylabel('Declination (degrees)')\n",
    "ax.set_title('Comparison of Astronomical Coordinates: J1950 vs J2000')\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()\n"
   ],
   "id": "2b8ddc0010750df8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There we have it! The plot shows the comparison between the J1950 and J2000 coordinates, with vector arrows indicating the changes in the coordinates due to the epoch transformation. This visualisation helps us understand the differences between the two epochs and how the coordinates have shifted over time. ",
   "id": "98b98ce6ce0968e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    End of Example\n",
    "  </hjson>"
   ],
   "id": "2ec0d43218affdbc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It's now your turn to apply the concepts you've learned above to the quantum mechanics grades dataset.",
   "id": "56f10594eaba066b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    Exercise: Data Transformation\n",
    "  </h1>"
   ],
   "id": "494803d187974105"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this activity, we'll take the grades from our quantum mechanics course and transform them into a standardised format for analysis. The grades are currently out of 120, and we want to convert them to a percentage scale (0-100) to ensure consistency and comparability.  First, start by importing the 'quantum_mechanics.grades' file into a DataFrame and inspecting the data to understand its structure and contents.",
   "id": "8b16ef02bf5d6884"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "eb1337f787996c81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, apply a transformation to the 'Grades' column to standardise the format. For this exercise, you should convert the grades to a percentage scale (0-100) to ensure consistency and comparability. The grades are currently out of 120, so you'll need to convert them to a percentage scale by dividing each grade by 120 and multiplying by 100. Update the 'Grades' column with the transformed values.",
   "id": "57c198612fad5eae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "a0acd94f293f08af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that you've transformed the grades into a standardised format, you can save the updated DataFrame to a new file for further analysis. Use the `to_csv()` method to write the DataFrame to a CSV file, ensuring that the transformed grades are preserved for future reference and analysis. We can also visually inspect the distribution of the original and transformed grades using a histogram to observe the changes in the data.\n",
   "id": "77741724ab2472bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "grades_df['Grades'].plot(kind='hist')",
   "id": "38fe7adf0ce4ff12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "grades_df['Scaled Grades'].plot(kind='hist', color='orange')",
   "id": "a13f3066f14751f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    End of Exercise\n",
    "  </h1>"
   ],
   "id": "ab94f1d385905bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Basic Plotting",
   "id": "647a42a62011062e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will now delve into the fundamentals of data visualisation, which helps convey complex insights in a clear and accessible way, making it easier to communicate findings to a broad audience. You'll learn how to create and interpret various types of visualisations to highlight patterns, trends, and relationships in your data.",
   "id": "dae0d42286c3089b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Note:** Make sure you have the latest versions of `matplotlib` and `seaborn` installed. Your data files should either be uploaded to Google Colab, in the same directory as your Jupyter notebook, or accessible through a URL to avoid path issues while loading files.\n",
   "id": "8be136319a240245"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import the required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "id": "fdf14b18aa5a1e08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's begin by looking at a variety of plots and how they can be used to represent different types of data. We will cover the following types of plots. As always, you are encouraged to run the code snippets in your own Python environment to see the plots in action. Let's start with the basics.\n",
   "id": "3f60f0c3af16f9b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Pie Chart\n",
    "Pie chats represent data in a circular graph where the size of each slice is proportional to the quantity it represents. Typically, they are divided into 2 or 3 segments as more segments can make it difficult to interpret the data. Let's create a pie chart using some random data."
   ],
   "id": "41c33cc0c2c332a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Random Data\n",
    "labels = ['A', 'B', 'C']\n",
    "sizes = [50, 30, 20]"
   ],
   "id": "af01bb21ac11a0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here I have provided some random data for the labels and sizes. The labels represent the categories and the sizes represent the values for each category. We can now create a pie chart using this data.\n",
   "id": "e82a42fa9f5e30c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# I begin by creating a pie chart using the `plt.pie()` function from the `matplotlib` library.\n",
    "plt.pie(sizes, labels=labels) # labels are the categories and sizes are the values for each category\n",
    "plt.title('Pie Chart Example') # Add a title to the chart\n",
    "plt.show() # This function displays the chart"
   ],
   "id": "e2ea1850f13d60be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is a very simple example, but it lacks some important information. For example, it is difficult to determine the exact value of each segment. We can add this information by using the `autopct` parameter in the `plt.pie()` function.",
   "id": "bfac2371500440b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%') # autopct shows the percentage on the chart\n",
    "plt.title('Pie Chart Example with Percentage') # Add a title to the chart\n",
    "plt.show()"
   ],
   "id": "e637f8f4a0efcd07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So we labelled this with the `autopct` parameter. Let's break down the `'%1.1f%%'` string:\n",
    "  - **%**: This indicates that we are using a format string.\n",
    "  - **1.1f**: \n",
    "    - **1**: The total number of digits before the decimal point.\n",
    "    - **.1**: Indicates that only one digit will be shown after the decimal point.\n",
    "    - **f**: Stands for floating-point number format.\n",
    "  - **%%**: Escapes the percentage symbol (`%`) so that it is displayed literally in the pie chart.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Is the plot a little small? Let's make it bigger."
   ],
   "id": "fcda7fb864788beb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 8)) # Set the size of the plot\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
    "plt.title('Pie Chart Example with Percentage') \n",
    "plt.show()"
   ],
   "id": "a1232e684af65d0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now I want to rotate the pie chart. We can do this by using the `startangle` parameter in the `plt.pie()` function.\n",
   "id": "9a5366a778b20b5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90) # startangle rotates the chart\n",
    "plt.title('Pie Chart Example with Percentage')\n",
    "plt.show()"
   ],
   "id": "68507cd5037d9d93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can also explode a segment of the pie chart. This can be done by using the `explode` parameter in the `plt.pie()` function.",
   "id": "3b8c33acd078ee41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "explode = (0, 0.1, 0) # This will explode the second segment\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, explode=explode) # explode the second segment\n",
    "plt.title('Pie Chart Example with Percentage')\n",
    "plt.show()"
   ],
   "id": "80293f2041a2ee75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can also add a legend to the pie chart. This can be done by using the `plt.legend()` function.\n",
   "id": "fbcd0f424734daba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, explode=explode)\n",
    "plt.title('Pie Chart Example with Percentage')\n",
    "plt.legend(loc='upper right') # Add a legend to the chart\n",
    "plt.show()\n"
   ],
   "id": "1448d6615700c526",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is looking nice! One final modification, is turning this into a donut chart. We can do this by using the `wedgeprops` parameter in the `plt.pie()` function.",
   "id": "24f96c46d6153044"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 8))  \n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, explode=explode, wedgeprops=dict(width=0.3)) # Create a donut chart\n",
    "plt.title('Donut Chart Example with Percentage')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ],
   "id": "e7edecbb35d1b0a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "All we did here was add the `wedgeprops=dict(width=0.3)` parameter to the `plt.pie()` function. This parameter specifies the width of the donut chart. A value of `0.3` will create a donut chart with a width of 30% of the radius. You can adjust this value to create a donut chart with a different width.\n",
   "id": "a7e9bd9c7af6bfa7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    Exercise: Pie Chart\n",
    "  </h1>"
   ],
   "id": "8b01f74622542d27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Part 1: Customising Labels\n",
    "1. Create a new pie chart with labels `['Red', 'Blue', 'Yellow']` and sizes `[40, 35, 25]`.\n",
    "2. Use the `autopct` parameter to show percentages with one decimal place.\n",
    "\n",
    "#### Part 2: Highlight a Different Segment\n",
    "1. Create a new pie chart with labels `['Group A', 'Group B', 'Group C']` and sizes `[60, 30, 10]`.\n",
    "2. Highlight the third segment of the pie chart using the `explode` parameter.\n",
    "\n",
    "#### Part 3: Donut Chart Variations\n",
    "1. Create a new donut chart using labels `['X', 'Y', 'Z']` and sizes `[45, 35, 20]`.\n",
    "2. Use the `wedgeprops` parameter to set the width of the donut chart to `0.5`.\n",
    "3. Add a custom colour palette of your choice."
   ],
   "id": "6790ba9d006d6f82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "7747b0977bf8da6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    End of Exercise\n",
    "  </h1>"
   ],
   "id": "96614845c90d5bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Bar Chart\n",
    "Bar charts are used to represent data in rectangular bars with lengths proportional to the values they represent. They are highly effective for comparing data across different categories. Let's create a bar chart using our randon data from above. To do this, we will use the `plt.bar()` function from the `matplotlib` library."
   ],
   "id": "6e487da1c43b4975"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, sizes) # Create a bar chart\n",
    "plt.title('Bar Chart Example')\n",
    "plt.xlabel('Categories') # Add a label to the x-axis\n",
    "plt.ylabel('Values') # Add a label to the y-axis\n",
    "plt.show()"
   ],
   "id": "d42e464ed1d5e888",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's try and rotate the plot. We can do this by using `plt.barh()` function.",
   "id": "d5b0cb53d3fb8e6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(labels, sizes) # Create a horizontal bar chart\n",
    "plt.title('Horizontal Bar Chart Example')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Categories')\n",
    "plt.show()"
   ],
   "id": "80d0ec6658f12d24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Want some different colours? We can do this by using the `color` parameter in the `plt.bar()` function.",
   "id": "15a5ce2044862217"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, sizes, color=['red', 'green', 'blue']) # Change the colour of the bars\n",
    "plt.title('Bar Chart Example')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Values')\n",
    "plt.show()"
   ],
   "id": "ef956f383c5459ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If you want it all red, you can use the `color` parameter with a single colour.",
   "id": "1d11626086ecba5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, sizes, color='red') # Change the colour of the bars\n",
    "plt.title('Bar Chart Example')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Values')\n",
    "plt.show()"
   ],
   "id": "80d5fdf4b6202b6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Perhaps the spacing is a little off. We can adjust this by using the `width` parameter in the `plt.bar()` function.",
   "id": "d97c980481d9d31c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, sizes, color='red', width=0.5) # Change the width of the bars\n",
    "plt.title('Bar Chart Example')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Values')\n",
    "plt.show()"
   ],
   "id": "73661bdb94e81a72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Say we have two groups of data. We can plot these side-by-side by using the `plt.bar()` function twice! Below, all I have done is create two groups of data and plotted them side-by-side.",
   "id": "1194a47dc6d88c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categories = ['A', 'B', 'C']  # Categories for the common Y-axis\n",
    "values1 = [10, 20, 30]  # Values for Group 1\n",
    "values2 = [-15, -25, -35]  # Values for Group 2 (negative for opposite side)\n",
    "plt.barh(categories, values1, label='Group 1')  # Plot horizontal bars for Group 1\n",
    "plt.barh(categories, values2, label='Group 2')  # Plot horizontal bars for Group 2\n",
    "plt.xlabel('Values') \n",
    "plt.ylabel('Categories') \n",
    "plt.title('Side-by-Side Bar Chart Example')  \n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "348d285d9b2460d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's now instead stack the two groups of data. Once again, we create two bars using the `plt.bar()` function, but this time we add the `bottom` parameter to the second bar. This parameter specifies the y-coordinate of the bottom of the bars. Just note that we assigned group 2 to be negative above, so we need to convert these values to positive before stacking.",
   "id": "ecfef52a75a90ee9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert values2 to positive for stacking\n",
    "values2 = np.abs(values2)\n",
    "\n",
    "# Create a stacked vertical bar chart\n",
    "plt.bar(categories, values1, label='Group 1')  # Plot bars for Group 1\n",
    "plt.bar(categories, values2, bottom=values1, label='Group 2')  # Stacked bars for Group 2 on top of Group 1\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Stacked Bar Chart Example')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "9a84e8654098deb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "What about three groups of data clustered together? We can do this by using the `plt.bar()` function three times! Below, I have created three groups of data and plotted them together.",
   "id": "19d0142195027423"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categories = ['A', 'B', 'C']  # Categories for the common X-axis\n",
    "values1 = [10, 20, 30]  # Values for Group 1\n",
    "values2 = [15, 25, 35]  # Values for Group 2\n",
    "values3 = [20, 30, 40]  # Values for Group 3\n",
    "bar_width = 0.25  # Width of the bars\n",
    "# I'm going to start adding some spaces between my code for better readability - consider this for your workbook too!\n",
    "\n",
    "plt.bar(np.arange(len(categories)), values1, width=bar_width, label='Group 1')  # Plot bars for Group 1\n",
    "plt.bar(np.arange(len(categories)) + bar_width, values2, width=bar_width, label='Group 2')  # Plot bars for Group 2\n",
    "plt.bar(np.arange(len(categories)) + 2 * bar_width, values3, width=bar_width, label='Group 3')  # Plot bars for Group 3\n",
    "# Another space for readability. Also, don't forget to adequately comment your code! ChatGPT or similar is obvious, so use your own words!\n",
    "\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Clustered Bar Chart Example')\n",
    "plt.xticks(np.arange(len(categories)) + bar_width, categories)  # We set + bar_width to centre the labels\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "d56944294bad360b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "These look great. Okay, let's try some exercises.",
   "id": "df3c3e65271b431f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    Exercise: Bar Chart\n",
    "  </h1>"
   ],
   "id": "c8cc9ff2085d3a30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Part 1: Create a Horizontal Bar Chart\n",
    "1. Create a new horizontal bar chart using labels `['Apple', 'Banana', 'Cherry']` and values `[15, 30, 20]`.\n",
    "2. Set the bar colours to `['red', 'yellow', 'pink']`.\n",
    "\n",
    "#### Part 2: Change Bar Colours and Add a Legend\n",
    "1. Create a new grouped bar chart using labels `['G1', 'G2', 'G3']` and values `[10, 20, 15]` for the first group and `[12, 18, 25]` for the second group.\n",
    "2. Assign different colours to each group and add a legend to distinguish between them.\n",
    "\n",
    "#### Part 3: Adjust Bar Width\n",
    "1. Create a new vertical bar chart with labels `['A', 'B', 'C']` and values `[40, 25, 35]`.\n",
    "2. Adjust the `width` parameter to `0.3`.\n",
    "3. Change the bar colours to use a single colour, such as `skyblue`."
   ],
   "id": "cbb4ada1283fce6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "5b1a2a45ed0867b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    End of Exercise\n",
    "  </h1>"
   ],
   "id": "ae180d8f779d5ab7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Line Chart\n",
    "Line charts are used to represent data in a series of data points connected by straight lines. They are useful for showing trends over time. Let's create a line chart using some random data. To do this, we will use the `plt.plot()` function from the `matplotlib` library. First I need some random data. I do this by using the `np.arange()` function from the `numpy` library and the `np.random.randint()` function to generate random integers."
   ],
   "id": "b1c3d0079c86ea2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Random Data\n",
    "x = np.arange(10)  # This is an ordered array from 0 to 9\n",
    "y = np.random.randint(0, 10, 10)  # We don't require ordered data for the Y-axis, so we can use random integers!\n",
    "\n",
    "plt.plot(x, y)  # This function creates a line chart\n",
    "plt.title('Line Chart Example')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()"
   ],
   "id": "d8f51e1d7cd94bf4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's add some markers to the line chart. We can do this by using the `marker` parameter in the `plt.plot()` function.",
   "id": "1cc67ca31011d45d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(x, y, marker='o')  # Add markers to the line chart\n",
    "plt.title('Line Chart Example with Markers')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()"
   ],
   "id": "16ff43653782e3e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can change the size and colour of the markers by using the `markersize` and `markerfacecolor` parameters in the `plt.plot()` function.",
   "id": "d686f8c45188a3d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(x, y, marker='o', markersize=10, markerfacecolor='red')  # Change the size and colour of the markers\n",
    "plt.title('Line Chart Example with Markers')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()"
   ],
   "id": "61e1d73dc2d47798",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If you'd like a different colour for the line, you can use the `color` parameter in the `plt.plot()` function.",
   "id": "cf0e02dd6628e20b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(x, y, marker='o', markersize=10, markerfacecolor='red', color='green')  # Change the colour of the line\n",
    "plt.title('Line Chart Example with Markers')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()"
   ],
   "id": "cf439082c4025f40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's add a grid to the line chart. We can do this by using the `plt.grid()` function.",
   "id": "7692a261780f2f59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(x, y, marker='o', markersize=10, markerfacecolor='red', color='green')\n",
    "plt.title('Line Chart Example with Markers and Grid')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.grid(True)  # Add a grid to the chart\n",
    "plt.show()"
   ],
   "id": "619efe2fcadbf183",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "What if we add a solid colour to the area under the line? We can do this by using the `plt.fill_between()` function.\n",
   "id": "dc68f66a4d07f026"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(x, y, marker='o', markersize=10, markerfacecolor='red', color='green')\n",
    "plt.fill_between(x, y, color='green', alpha=0.2)  # Add a solid colour to the area under the line\n",
    "plt.title('Line Chart Example with Markers and Fill')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()"
   ],
   "id": "55801128d7628245",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Want a solid colour above the line? We can do this by using the `y2` parameter in the `plt.fill_between()` function. The `y2=max(y)+1` sets the upper boundary of the fill to just above the highest y value, allowing the area above the line to be filled with a different colour.",
   "id": "d7a6576d709ccd31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(x, y, marker='o', markersize=10, markerfacecolor='red', color='green')\n",
    "plt.fill_between(x, y, color='green', alpha=0.2)  # Area below the line\n",
    "plt.fill_between(x, y, y2=max(y)+1, color='blue', alpha=0.2)  # Area above the line\n",
    "plt.title('Line Chart with Markers and Fill Above/Below the Line')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()"
   ],
   "id": "b6fa3126ddbb735a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Perhaps we want to apply the solid colour just to a region between x = 2 and 4. We can do this by using the `where` parameter in the `plt.fill_between()` function.",
   "id": "f1cf568a2bd71059"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(x, y, marker='o', markersize=10, markerfacecolor='red', color='green')\n",
    "plt.fill_between(x, y, where=[2 <= i <= 4 for i in x], color='red', alpha=0.2)  # Area between x=2 and x=4\n",
    "plt.title('Line Chart with Markers and Fill Between x=2 and x=4')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()"
   ],
   "id": "599107fd50bcd1ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Want a second line on the chart? We can do this by using the `plt.plot()` function twice! Below, I have created two lines and plotted them together.",
   "id": "48984549aaae695e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y2 = np.random.randint(0, 10, 10)  # Random data for the second line\n",
    "\n",
    "plt.plot(x, y, marker='o', color='green', label='Line 1')  # Plot the first line\n",
    "plt.plot(x, y2, marker='o', color='red', label='Line 2')  # Plot the second line\n",
    "plt.title('Line Chart with Two Lines')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "b18d82cfb25105f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Great! Let's try some exercises.",
   "id": "331edb931e2ba6fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    Exercise: Line Chart\n",
    "  </h1>"
   ],
   "id": "ed61a7cdb92922aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Part 1: Create a New Line Chart with More Data Points\n",
    "1. Create a line chart using `x = np.arange(15)` and `y = np.random.randint(0, 20, 15)`.\n",
    "2. Set the line colour to blue and add circular markers (`'o'`) at each data point.\n",
    "\n",
    "#### Part 2: Add a Second Line\n",
    "1. Create a second set of data using `y2 = np.random.randint(0, 20, 15)`.\n",
    "2. Plot both `y` and `y2` on the same line chart using different colours and line styles.\n",
    "3. Add a legend to label each line.\n",
    "\n",
    "#### Part 3: Highlight Specific Data Points\n",
    "1. Create a line chart using `x = np.arange(10)` and `y = np.random.randint(0, 15, 10)`.\n",
    "2. Use the `marker='D'` and `markersize=8` parameters to add diamond markers at each data point.\n",
    "3. Change the marker colour to `orange`."
   ],
   "id": "57a0d3cb42979755"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "bd7d1d367b6a5ef1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    End of Exercise\n",
    "  </h1>"
   ],
   "id": "9a3c81d185e364c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Scatter Plot\n",
    "Scatter plots are used to represent data as a collection of points. They are useful for visualising the relationship between two variables. Let's create a scatter plot using some random data. To do this, we will use the `plt.scatter()` function from the `matplotlib` library. First I need some random data. I do this by using the `np.random.randn()` function from the `numpy` library to generate random numbers."
   ],
   "id": "e8c44a2b6e57f235"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Random Data\n",
    "x = np.random.randn(100)  # Random data for the x-axis\n",
    "y = np.random.randn(100)  # Random data for the y-axis\n",
    "\n",
    "plt.scatter(x, y)  # This function creates a scatter plot\n",
    "plt.title('Scatter Plot Example')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()"
   ],
   "id": "3cccc715f129ce55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "What about a line of best fit? We can do this by using the `np.polyfit()` function from the `numpy` library to calculate the coefficients of the line of best fit. We can then use the `np.poly1d()` function",
   "id": "942b896183278626"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "coefficients = np.polyfit(x, y, 1)\n",
    "line = np.poly1d(coefficients)"
   ],
   "id": "af01fd7716e89b81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The line `coefficients = np.polyfit(x, y, 1)` fits a linear regression (line of best fit) to the data by finding the slope and intercept, while `line = np.poly1d(coefficients)` creates a polynomial function representing that line, which can be used to plot the best fit across the data points.",
   "id": "8654ebb31bb54dce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(x, line(x), color='red')  # This is our line of best fit\n",
    "plt.title('Scatter Plot with Line of Best Fit')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()"
   ],
   "id": "6757cd076a4c30b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's add a second set of random data to the scatter plot. We can do this by using the `plt.scatter()` function twice! Below, I have created two sets of random data and plotted them together.\n",
   "id": "5b2a4bdfde7d9721"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x2 = np.random.randn(100)  # Random data for the x-axis\n",
    "y2 = np.random.randn(100)  # Random data for the y-axis\n",
    "\n",
    "plt.scatter(x, y, color='red', label='Group 1')  # Plot the first set of data\n",
    "plt.scatter(x2, y2, color='blue', label='Group 2')  # Plot the second set of data\n",
    "plt.title('Scatter Plot with Two Groups')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "44f4762de241c8db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Want to change the size of the markers? We can do this by using the `s` parameter in the `plt.scatter()` function.\n",
   "id": "e15241a53b81baae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.scatter(x, y, s=100)  # Change the size of the markers\n",
    "plt.title('Scatter Plot Example with Larger Markers')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()"
   ],
   "id": "4cba8678ec2fe4e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If there's a third variable, we can change the colour of the markers. We can do this by using the `c` parameter in the `plt.scatter()` function.\n",
   "id": "a1b7f9b26308bb0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "z = np.random.randint(50, 150, size=100)  # Random values for marker size\n",
    "plt.scatter(x, y, c=z, cmap='cool')  # Change the colour of the markers\n",
    "plt.title('Scatter Plot Example with Colour')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.colorbar()  # Add a colour bar\n",
    "plt.show()"
   ],
   "id": "888cfcda0732ebd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Or perhaps change the size of the markers based on the third variable. We can do this by using the `s` parameter in the `plt.scatter()` function.\n",
   "id": "99013af571c4f3bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.scatter(x, y, s=z)  # Change the size of the markers (here I multiplied by 100 for better visualisation)\n",
    "plt.title('Scatter Plot Example with Size')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()"
   ],
   "id": "dd31e33777186b3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's try some exercises.",
   "id": "4365630fb4d2702f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    Exercise: Scatter Plot\n",
    "  </h1>"
   ],
   "id": "2919262571c421d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Part 1: Create a Scatter Plot\n",
    "1. Create a scatter plot using `x = np.random.randn(50)` and `y = np.random.randn(50)`.\n",
    "2. Set the marker size to `50` and the colour to `green`.\n",
    "\n",
    "#### Part 2: Add a Second Set of Data\n",
    "1. Create a second scatter plot using `x2 = np.random.randn(50)` and `y2 = np.random.randn(50)`.\n",
    "2. Plot both datasets on the same chart using different colours.\n",
    "3. Add labels and a legend to distinguish between the two groups.\n",
    "\n",
    "#### Part 3: Create a Colour-Coded Scatter Plot\n",
    "1. Create a new scatter plot using `x = np.random.randn(100)`, `y = np.random.randn(100)`, and a third variable `z = np.random.randint(10, 100, 100)`.\n",
    "2. Use the `c=z` parameter and set `cmap='plasma'`.\n",
    "3. Add a colour bar using `plt.colorbar()` to indicate the range of values."
   ],
   "id": "46357ae9ce8e5b8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "62a760402c84e0be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    End of Exercise\n",
    "  </h1>"
   ],
   "id": "51f8d2d1f7126cbd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Histogram\n",
    "Histograms are used to represent the distribution of a continuous variable. They are useful for visualising the frequency of data points within a specific range. Let's create a histogram using some random data. To do this, we will use the `plt.hist()` function from the `matplotlib` library. First I need some random data. I do this by using the `np.random.randn()` function from the `numpy` library to generate random numbers."
   ],
   "id": "19f3a3f9629bf190"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Random Data\n",
    "data = np.random.randn(1000)  # Random data for the histogram where 1000 is the number of data points\n",
    "\n",
    "plt.hist(data)  # This function creates a histogram\n",
    "plt.title('Histogram Example')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "5eb202372441e8fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "What about changing the number of bins to control the granularity of the histogram? We can do this by using the `bins` parameter in the `plt.hist()` function.\n",
   "id": "a0aeec8010ff3339"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.hist(data, bins=20)  # Change the number of bins\n",
    "plt.title('Histogram Example with 20 Bins')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "e8245644c69e1d8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Instead of bars, we can use step-type histograms. We can do this by using the `histtype` parameter in the `plt.hist()` function.",
   "id": "933fa95740dfb6ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.hist(data, bins=20, histtype='step')  # Use a step-type histogram\n",
    "plt.title('Histogram Example with Step')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "c9fc753c0af90a61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Another histtype is 'stepfilled'. This is similar to 'step', but the area between the steps is filled. We can do this by using the `histtype` parameter in the `plt.hist()` function.\n",
   "id": "22d00d6b08186332"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.hist(data, bins=20, histtype='stepfilled')  # Use a stepfilled histogram\n",
    "plt.title('Histogram Example with Step Filled')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "cf2c50a9443d7221",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can add a KDE (Kernel Density Estimate) to the histogram. A KDE is a smoothed version of the histogram that can provide additional insights into the data distribution. We need the Seaborn library for this. We can do this by using the `sns.histplot()` function from the `seaborn` library instead of the `plt.hist()` function. ",
   "id": "802c8e39881c5439"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.histplot(data, bins=20, kde=True)  # Use seaborn's histplot for KDE\n",
    "plt.title('Histogram Example with KDE')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "67936356563f73b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's add a second set of random data to the histogram. We can do this by using the `plt.hist()` function twice! Below, I have created two sets of random data and plotted them together.\n",
   "id": "ac4305705ec1bc5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data2 = np.random.randn(100)  # Random data for the second histogram\n",
    "\n",
    "plt.hist(data, bins=20, alpha=0.5, label='Group 1')  # Plot the first set of data with an alpha value of 0.5 to make it transparent\n",
    "plt.hist(data2, bins=20, alpha=0.5, label='Group 2')  # Plot the second set of data\n",
    "plt.title('Histogram with Two Groups')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "c3730da8b82ac74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Notice that the second set of data is not as tall as the first set. This is because the number of data points in the second set is less than the first set. We can normalise the histogram to account for this. We can do this by using the `density` parameter in the `plt.hist()` function. Keep in mind that you should adequately communicate this normalisation in your visualisation!",
   "id": "5f24959a45c5ee9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.hist(data, bins=20, alpha=0.5, label='Group 1', density=True)  # Normalise the first set of data\n",
    "plt.hist(data2, bins=20, alpha=0.5, label='Group 2', density=True)  # Normalise the second set of data\n",
    "plt.title('Normalised Histogram with Two Groups')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "af7274e424c05f9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "What about a stacked histogram? We can do this by using the `stacked` parameter in the `plt.hist()` function.",
   "id": "189be5e2f94b6486"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.hist([data, data2], bins=20, alpha=0.5, label=['Group 1', 'Group 2'], stacked=True)  # Create a stacked histogram\n",
    "plt.title('Stacked Histogram with Two Groups')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "cca3024405ae3d0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Prefer to go horizontal? We can do this by using the `orientation` parameter in the `plt.hist()` function.\n",
   "id": "6e14f2bf00c22081"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.hist(data, bins=20, orientation='horizontal')  # Create a horizontal histogram\n",
    "plt.title('Horizontal Histogram Example')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Values')\n",
    "plt.show()"
   ],
   "id": "61e6dc28f700db2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "What about a cumulative histogram? This plot is a way to show the running total of data points as you move across the values. Instead of displaying the number of data points in each bin, a cumulative histogram shows how many data points fall *up to* a certain value. You can create a cumulative histogram by using the `cumulative=True` parameter in the `plt.hist()` function.",
   "id": "42678883d4444fae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.hist(data, bins=20, cumulative=True)  # Create a cumulative histogram\n",
    "plt.title('Cumulative Histogram Example')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Cumulative Frequency')\n",
    "plt.show()\n"
   ],
   "id": "da4d0f11c5f44d6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    Exercise: Histogram\n",
    "  </h1>"
   ],
   "id": "7ea4e9c4331c5be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Part 1: Create a Histogram with Different Bin Sizes\n",
    "1. Generate random data using `data = np.random.randn(500)`.\n",
    "2. Create a histogram with `30` bins.\n",
    "3. Set the bar colour to `lightcoral`.\n",
    "\n",
    "#### Part 2: Create a Cumulative Histogram\n",
    "1. Generate a new set of random data using `data = np.random.randn(1000)`.\n",
    "2. Create a cumulative histogram using `plt.hist(data, cumulative=True, bins=25)`.\n",
    "3. Set the bar colour to `steelblue`.\n",
    "\n",
    "#### Part 3: Create a Stacked Histogram\n",
    "1. Generate two different sets of random data: `data1 = np.random.randn(500)` and `data2 = np.random.randn(500)`.\n",
    "2. Plot them together using the `stacked=True` parameter.\n",
    "3. Set different colours for each dataset using `color=['salmon', 'skyblue']`."
   ],
   "id": "f6c81489d2bc9f8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "f27a72f554c053e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    End of Exercise\n",
    "  </h1>"
   ],
   "id": "19d19caa75286a6c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Visualising Uncertainty and Spread\n",
    "\n",
    "In previous sessions, we discussed how to calculate and interpret uncertainty and spread using statistical measures such as standard deviation, variance, and confidence intervals. Now, we will look at how to visualise these uncertainties in your plots to make your data communication more effective.\n",
    "\n",
    "#### Why Include Uncertainty in Visualisations?\n",
    "\n",
    "When working with real-world data, there’s almost always some uncertainty in measurements or estimates. Including this information in your visualisations helps convey the reliability of your findings and avoids misleading interpretations. For example:\n",
    "\n",
    "- Error bars show how much a particular value might vary due to measurement or sampling errors.\n",
    "- Shaded regions around a line indicate confidence intervals or uncertainty bands, giving a clearer picture of the range of possible outcomes.\n",
    "- Boxplots and violin plots are used to represent the spread of the data, helping to visualise distributions and identify outliers."
   ],
   "id": "508c41a88faf597b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### How Do We Add Uncertainty to Visualisations?\n",
    "\n",
    "Below are some common methods for visualising uncertainty and spread. We'll start with error bars in bar charts and line charts. "
   ],
   "id": "e138b61ae0aeda7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categories = ['A', 'B', 'C']\n",
    "values = [5, 7, 3] # Sample mean values\n",
    "std_dev = [0.5, 0.7, 0.3]  # Standard deviation values calculates elsewhere\n",
    "\n",
    "plt.bar(categories, values, yerr=std_dev, capsize=5, color='skyblue')\n",
    "plt.title('Bar Chart with Error Bars')\n",
    "plt.show()"
   ],
   "id": "7555235b025abcb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Another options is shaded regions for line charts. We can do this by using the `plt.fill_between()` function. Below, I have created a line chart with shaded regions, which represent the uncertainty in the data. This could be either spread (e.g., standard deviation) or uncertainty around a central value.\n",
   "id": "1a78c898d2c9c107"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = np.linspace(0, 10, 100) # 100 points from 0 to 10\n",
    "y = np.sin(x) # sine function\n",
    "error = 0.1 + 0.1 * np.sqrt(x) # here I am just creating some random error values\n",
    "\n",
    "plt.plot(x, y, color='blue')\n",
    "plt.fill_between(x, y - error, y + error, color='blue', alpha=0.2) # we set above and below the line\n",
    "plt.title('Line Chart with Shaded Regions')\n",
    "plt.show()"
   ],
   "id": "f39ce5a222da5956",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can also use boxplots to visualise the spread of data. Below, I have created a boxplot to show the spread of data for different categories. I will start by creating some random data and saving it in a DataFrame. I will then use the `sns.boxplot()` function from the `seaborn` library to create the boxplot. This automatically calculates the quartiles and the interquartile range (IQR) for each category.",
   "id": "917a0f1e1a4b1db3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = pd.DataFrame({\n",
    "    'Category': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],\n",
    "    'Values': [5, 7, 6, 8, 7, 6, 4, 3, 5]\n",
    "})\n",
    "\n",
    "sns.boxplot(x='Category', y='Values', data=data)\n",
    "plt.title('Boxplot of Categories')\n",
    "plt.show()"
   ],
   "id": "daaf1c2543f32383",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The boxplot components:\n",
    "- The line inside the box represents the median.\n",
    "- The box represents the interquartile range (IQR), which contains the middle 50% of the data.\n",
    "- The whiskers extend to the minimum and maximum values within 1.5 times the IQR.\n",
    "- Any points outside the whiskers are considered outliers.\n"
   ],
   "id": "b01b80e43818c578"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's create some error bars on scatter plots. We can do this by using the `plt.errorbar()` function. Below, I have created a scatter plot with error bars to show the uncertainty or spread in the data.\n",
   "id": "ae84f604a6489636"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = np.arange(10)  # 10 data points\n",
    "y = np.random.randn(10)  # Random y-values\n",
    "error = np.random.rand(10) * 0.5  # Random error values\n",
    "\n",
    "plt.errorbar(x, y, yerr=error, fmt='o', color='green', ecolor='red', capsize=5)  # The yerr parameter sets the error values\n",
    "plt.title('Scatter Plot with Error Bars')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()"
   ],
   "id": "78348884e3f465ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can change the width of the error bars by using the `capsize` parameter in the `plt.errorbar()` function. Below, I have increased the width of the error bars to make them more visible.",
   "id": "5ab97f466f87a1b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.errorbar(x, y, yerr=error, fmt='o', color='green', ecolor='red', capsize=10)  # Increase the width of the error bars\n",
    "plt.title('Scatter Plot with Error Bars')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()"
   ],
   "id": "4f04b46fc97c1f06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's remove the markers and make the colour lighter. We can do this by using the `fmt` parameter in the `plt.errorbar()` function, and setting the `color` parameter to a lighter shade. We also set the cap size to 0 to remove the caps.\n",
   "id": "7be8fc7f94a85000"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.errorbar(x, y, yerr=error, fmt='o', color='black', ecolor='lightgray', elinewidth=3, capsize=0)\n",
    "plt.title('Scatter Plot with Error Bars')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()"
   ],
   "id": "290dd050b669e48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the previous examples, I either used made up errors or used **standard deviation (SD)**. Recall that SD is a measure of **spread** — it tells us how much individual data points vary around the mean. This is useful for understanding variability within a single dataset.\n",
    "\n",
    "However, other measures can be used to indicate **uncertainty**:\n",
    "\n",
    "- **Standard Error (SE)**: Measures the variability of the **sample mean** rather than the spread of the data. It decreases as sample size increases, showing increased confidence in the mean. You can calculate it as `SE = SD / sqrt(n)` where `n` is the sample size.\n",
    "  \n",
    "- **Confidence Intervals (CI)**: Show a range of values within which the true mean is likely to fall, providing a clearer view of **mean reliability**. Refer to the lecture for more details on how to calculate CI.\n",
    "\n",
    "- **Other Measures**: Depending on your data, you might choose **percentiles**, **prediction intervals**, or **bootstrap estimates**.\n",
    "\n",
    "**Choosing the Right Measure**\n",
    "\n",
    "The choice of measure should match your **data context** and the **message** you want to convey:\n",
    "\n",
    "- Use **Standard Deviation** to show **data spread**.\n",
    "- Use **Standard Error** or **Confidence Intervals** to show **uncertainty around a mean**.\n",
    "\n",
    "**Key Tips:**\n",
    "\n",
    "1. **Always Label Your Error Bars** — Specify if you’re using SD, SE, or CI in your legend or title.\n",
    "2. **Choose the Right Measure** — Decide if you want to show data spread or uncertainty.\n",
    "3. **Avoid Misinterpretation** — Using the wrong measure can mislead your audience.\n",
    "\n",
    "The following image illustrates the differences between **spread** (variability within the sample) and **error** (variability of the sample mean):\n",
    "\n",
    "<img src=\"https://mjcowley.github.io/images/spread_error.png\" alt=\"Spread vs Error\" width=\"600\"/>\n",
    "\n",
    "1. **Top Panel – Population Distribution:**\n",
    "   - Shows the **population mean** and **standard deviation (SD)**, representing the spread of the entire population.\n",
    "   - The SD tells us how much the individual data points are likely to vary from the mean.\n",
    "\n",
    "2. **Middle Panel – Sample Distribution:**\n",
    "   - Displays a random sample taken from the population.\n",
    "   - The **sample mean** is different from the true population mean due to sampling variability.\n",
    "   - The **sample standard deviation** is used here to estimate the spread of the sample values.\n",
    "\n",
    "3. **Bottom Panel – Sampling Distribution of the Mean:**\n",
    "   - This panel highlights **standard error (SE)**.\n",
    "   - SE measures the variability of the **sample mean** if we repeatedly took samples from the population.\n",
    "   - As more samples are collected, SE decreases, showing increased confidence in our mean estimate.\n",
    "   - In contrast to SD, SE reflects **uncertainty** in the estimate of the mean rather than the spread of individual data points.\n",
    "\n",
    "By understanding these differences, you can make better decisions about which measure to use when plotting your data and effectively communicate your findings. Let's end with an exercise.\n",
    "\n"
   ],
   "id": "755183c52c2d273b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    Exercise: Visualising Uncertainty and Spread\n",
    "  </h1>"
   ],
   "id": "b33c11edf886decb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this activity, you will create a line plot of global temperature anomalies over time. You will also calculate a rolling standard deviation to show how temperature variability changes over the years. Let's make use of our global temperature dataset to visualise these trends. Here's what you need to do:\n",
    "\n",
    "1. **Read the Data:**\n",
    "   - Use the provided CSV file (`global_temp_anomalies.csv`), which contains two columns:\n",
    "     - `Year`: The year of observation.\n",
    "     - `Anomaly`: The global temperature anomaly for that year.\n",
    "\n",
    "2. **Create a Line Plot:**\n",
    "   - Plot a line graph of the `Anomaly` values against the `Year` to visualise the trend in temperature anomalies over time. Michael showed an example of this in an earlier lecture.\n",
    "\n",
    "3. **Calculate a Rolling Standard Deviation:**\n",
    "   - Use a rolling window (e.g., 10 years) to calculate the standard deviation of the anomalies. You can do this using the `rolling()` function in `pandas`. I provide it here for you: `rolling_std = data['Anomaly'].rolling(window=10).std()`.\n",
    "   - Add this rolling standard deviation as a shaded region around the line plot to show how variability has changed over time.\n",
    "\n",
    "4. **Customise the Plot:**\n",
    "   - Add labels, a title, and a legend to make your visualisation more informative.\n",
    "   \n",
    "5. **Interpret the Visualisation:**\n",
    "   - What does the trend in the anomalies tell you about global temperature changes?"
   ],
   "id": "d85f95aa2bda528e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"text-align: center;\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"background: white; color: black; border: 0; padding: 20px 0; display: inline-block; width: 90%; box-sizing: border-box;\">\n",
    "    End of Exercise\n",
    "  </h1>"
   ],
   "id": "97486294900aff3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Further Exploration and Practice\n",
    "\n",
    "Congratulations on completing your introuction to Python in Google Colab! You've made a significant first step towards mastering Python for data analysis and visualisation. To bolster your understanding and prepare for what's ahead, consider exploring the following resources:\n",
    "\n",
    "- **Codecademy's Python Course:** An interactive platform for learning Python with a hands-on approach. Perfect for reinforcing what you've learned and building on it. [Codecademy's Python Course](https://www.codecademy.com/learn/learn-python-3).\n",
    "- **Kaggle's Python Course:** Focused on data science applications, this course is ideal for those looking to delve into data manipulation and analysis. [Kaggle's Python Course](https://www.kaggle.com/learn/python).\n",
    "- **Real Python:** Provides a wealth of tutorials and exercises, beneficial for all levels of Python developers. [Real Python](https://realpython.com/).\n",
    "- **Python Official Documentation:** For in-depth learning, nothing beats the [Python official documentation](https://docs.python.org/3/). Use it to clarify concepts and learn about new features of the language.\n",
    "- **Towards Data Science - Data Cleaning:** This guide offers practical tips and examples for cleaning your data in Python. [Towards Data Science - Data Cleaning](https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4)\n",
    "- **Kaggle Data Cleaning Challenge:** Kaggle's micro-courses and challenges are great for hands-on practice in data cleaning and manipulation. [Kaggle Data Cleaning Challenge](https://www.kaggle.com/learn/data-cleaning)\n",
    "- **Pandas Documentation:** Deep dive into the Pandas documentation to understand the full capabilities of this powerful library. [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- **Data Science Central - Data Preparation:** Explore articles and tutorials that discuss various aspects of data preparation and best practices. [Data Science Central - Data Preparation](https://www.datasciencecentral.com/profiles/blogs/data-preparation)\n",
    "- **Python Graph Gallery: Visualisation Catalogue:** This gallery provides a wide range of visualisation examples and code snippets for inspiration. [Python Graph Gallery](https://www.python-graph-gallery.com/)\n",
    "\n",
    "Keep practicing and experimenting with code to solidify your skills. Happy coding!\n"
   ],
   "id": "13c9f7abd9389723"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
